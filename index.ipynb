{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? no\n",
    "- Is there a high variance? yes\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Aplly dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; import some packages/modules you plan to use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; load and preview the dataset\n",
    "df= pd.read_csv('Bank_complaints.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "random.seed(123)\n",
    "df = df.sample(10000)\n",
    "df.index = range(10000)\n",
    "\n",
    "\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "product_cat = le.transform(product) \n",
    "\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "product_onehot = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_results, product_onehot, test_size=1500, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.9828 - acc: 0.1161 - val_loss: 1.9515 - val_acc: 0.1230\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.9412 - acc: 0.1540 - val_loss: 1.9305 - val_acc: 0.1560\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.9213 - acc: 0.2019 - val_loss: 1.9147 - val_acc: 0.1990\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.9027 - acc: 0.2385 - val_loss: 1.8981 - val_acc: 0.2190\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.8829 - acc: 0.2653 - val_loss: 1.8794 - val_acc: 0.2370\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.8601 - acc: 0.2931 - val_loss: 1.8572 - val_acc: 0.2620\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.8328 - acc: 0.3160 - val_loss: 1.8303 - val_acc: 0.2750\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.7997 - acc: 0.3412 - val_loss: 1.7974 - val_acc: 0.3160\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.7608 - acc: 0.3724 - val_loss: 1.7585 - val_acc: 0.3450\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.7164 - acc: 0.3977 - val_loss: 1.7137 - val_acc: 0.3620\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.6675 - acc: 0.4295 - val_loss: 1.6654 - val_acc: 0.3920\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.6154 - acc: 0.4560 - val_loss: 1.6147 - val_acc: 0.4200\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.5606 - acc: 0.4788 - val_loss: 1.5615 - val_acc: 0.4520\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.5046 - acc: 0.5037 - val_loss: 1.5077 - val_acc: 0.4740\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4483 - acc: 0.5272 - val_loss: 1.4530 - val_acc: 0.4990\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3926 - acc: 0.5472 - val_loss: 1.4004 - val_acc: 0.5180\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3396 - acc: 0.5643 - val_loss: 1.3504 - val_acc: 0.5400\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.2891 - acc: 0.5792 - val_loss: 1.3047 - val_acc: 0.5650\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.2418 - acc: 0.5989 - val_loss: 1.2599 - val_acc: 0.5830\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.1978 - acc: 0.6149 - val_loss: 1.2196 - val_acc: 0.5990\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1573 - acc: 0.6292 - val_loss: 1.1832 - val_acc: 0.6190\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.1197 - acc: 0.6433 - val_loss: 1.1501 - val_acc: 0.6260\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0849 - acc: 0.6537 - val_loss: 1.1166 - val_acc: 0.6410\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0528 - acc: 0.6685 - val_loss: 1.0874 - val_acc: 0.6410\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0231 - acc: 0.6761 - val_loss: 1.0627 - val_acc: 0.6540\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9953 - acc: 0.6889 - val_loss: 1.0369 - val_acc: 0.6630\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9686 - acc: 0.6972 - val_loss: 1.0147 - val_acc: 0.6680\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9439 - acc: 0.7061 - val_loss: 0.9938 - val_acc: 0.6740\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9207 - acc: 0.7183 - val_loss: 0.9737 - val_acc: 0.6680\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8983 - acc: 0.7227 - val_loss: 0.9545 - val_acc: 0.6860\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8773 - acc: 0.7317 - val_loss: 0.9369 - val_acc: 0.6920\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8569 - acc: 0.7341 - val_loss: 0.9202 - val_acc: 0.6970\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8380 - acc: 0.7412 - val_loss: 0.9063 - val_acc: 0.6890\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8195 - acc: 0.7460 - val_loss: 0.8905 - val_acc: 0.6940\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8019 - acc: 0.7505 - val_loss: 0.8776 - val_acc: 0.7000\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.7856 - acc: 0.7541 - val_loss: 0.8642 - val_acc: 0.6990\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7697 - acc: 0.7599 - val_loss: 0.8516 - val_acc: 0.7000\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7541 - acc: 0.7619 - val_loss: 0.8435 - val_acc: 0.6920\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7402 - acc: 0.7649 - val_loss: 0.8289 - val_acc: 0.7090\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.7265 - acc: 0.7687 - val_loss: 0.8189 - val_acc: 0.7050\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7131 - acc: 0.7737 - val_loss: 0.8090 - val_acc: 0.7150\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7005 - acc: 0.7765 - val_loss: 0.8006 - val_acc: 0.7140\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.6884 - acc: 0.7817 - val_loss: 0.7918 - val_acc: 0.7120\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.6771 - acc: 0.7825 - val_loss: 0.7836 - val_acc: 0.7130\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.6659 - acc: 0.7884 - val_loss: 0.7777 - val_acc: 0.7240\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.6553 - acc: 0.7883 - val_loss: 0.7692 - val_acc: 0.7200\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.6455 - acc: 0.7928 - val_loss: 0.7653 - val_acc: 0.7210\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.6360 - acc: 0.7960 - val_loss: 0.7596 - val_acc: 0.7300\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.6272 - acc: 0.7984 - val_loss: 0.7529 - val_acc: 0.7330\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.6183 - acc: 0.8004 - val_loss: 0.7491 - val_acc: 0.7240\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.6099 - acc: 0.8023 - val_loss: 0.7403 - val_acc: 0.7340\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.6016 - acc: 0.8056 - val_loss: 0.7392 - val_acc: 0.7300\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.5946 - acc: 0.8067 - val_loss: 0.7332 - val_acc: 0.7340\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.5866 - acc: 0.8097 - val_loss: 0.7299 - val_acc: 0.7320\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.5796 - acc: 0.8120 - val_loss: 0.7237 - val_acc: 0.7350\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.5725 - acc: 0.8140 - val_loss: 0.7207 - val_acc: 0.7410\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.5661 - acc: 0.8164 - val_loss: 0.7178 - val_acc: 0.7320\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.5598 - acc: 0.8168 - val_loss: 0.7140 - val_acc: 0.7350\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.5536 - acc: 0.8184 - val_loss: 0.7123 - val_acc: 0.7360\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.5475 - acc: 0.8204 - val_loss: 0.7091 - val_acc: 0.7370\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.5413 - acc: 0.8224 - val_loss: 0.7057 - val_acc: 0.7430\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.5361 - acc: 0.8236 - val_loss: 0.7047 - val_acc: 0.7430\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.5306 - acc: 0.8263 - val_loss: 0.7025 - val_acc: 0.7400\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.5248 - acc: 0.8271 - val_loss: 0.7013 - val_acc: 0.7400\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.5197 - acc: 0.8312 - val_loss: 0.6991 - val_acc: 0.7420\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.5149 - acc: 0.8297 - val_loss: 0.6963 - val_acc: 0.7480\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.5100 - acc: 0.8308 - val_loss: 0.6951 - val_acc: 0.7420\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.5052 - acc: 0.8327 - val_loss: 0.6940 - val_acc: 0.7380\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.5006 - acc: 0.8325 - val_loss: 0.6904 - val_acc: 0.7470\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.4959 - acc: 0.8365 - val_loss: 0.6912 - val_acc: 0.7350\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.4911 - acc: 0.8375 - val_loss: 0.6907 - val_acc: 0.7340\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.4870 - acc: 0.8387 - val_loss: 0.6870 - val_acc: 0.7500\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4821 - acc: 0.8388 - val_loss: 0.6906 - val_acc: 0.7370\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.4783 - acc: 0.8401 - val_loss: 0.6839 - val_acc: 0.7400\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4738 - acc: 0.8416 - val_loss: 0.6843 - val_acc: 0.7470\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4700 - acc: 0.8441 - val_loss: 0.6825 - val_acc: 0.7460\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4659 - acc: 0.8459 - val_loss: 0.6865 - val_acc: 0.7440\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4613 - acc: 0.8504 - val_loss: 0.6811 - val_acc: 0.7540\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4584 - acc: 0.8467 - val_loss: 0.6800 - val_acc: 0.7460\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.4538 - acc: 0.8489 - val_loss: 0.6800 - val_acc: 0.7370\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4506 - acc: 0.8511 - val_loss: 0.6834 - val_acc: 0.7410\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4472 - acc: 0.8517 - val_loss: 0.6795 - val_acc: 0.7510\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.4425 - acc: 0.8540 - val_loss: 0.6770 - val_acc: 0.7550\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4395 - acc: 0.8549 - val_loss: 0.6770 - val_acc: 0.7510\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4359 - acc: 0.8565 - val_loss: 0.6784 - val_acc: 0.7480\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.4327 - acc: 0.8557 - val_loss: 0.6758 - val_acc: 0.7470\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4287 - acc: 0.8591 - val_loss: 0.6742 - val_acc: 0.7450\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4255 - acc: 0.8596 - val_loss: 0.6801 - val_acc: 0.7570\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4223 - acc: 0.8612 - val_loss: 0.6755 - val_acc: 0.7430\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4188 - acc: 0.8631 - val_loss: 0.6753 - val_acc: 0.7500\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4157 - acc: 0.8627 - val_loss: 0.6768 - val_acc: 0.7550\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4127 - acc: 0.8647 - val_loss: 0.6768 - val_acc: 0.7520\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4094 - acc: 0.8649 - val_loss: 0.6769 - val_acc: 0.7550\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.4061 - acc: 0.8667 - val_loss: 0.6835 - val_acc: 0.7460\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.4033 - acc: 0.8664 - val_loss: 0.6789 - val_acc: 0.7480\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.4000 - acc: 0.8688 - val_loss: 0.6740 - val_acc: 0.7530\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.3968 - acc: 0.8693 - val_loss: 0.6771 - val_acc: 0.7510\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.3943 - acc: 0.8716 - val_loss: 0.6738 - val_acc: 0.7510\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.3911 - acc: 0.8724 - val_loss: 0.6763 - val_acc: 0.7550\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.3881 - acc: 0.8736 - val_loss: 0.6867 - val_acc: 0.7440\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.3855 - acc: 0.8765 - val_loss: 0.6737 - val_acc: 0.7570\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.3822 - acc: 0.8769 - val_loss: 0.6756 - val_acc: 0.7540\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.3795 - acc: 0.8776 - val_loss: 0.6761 - val_acc: 0.7520\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.3769 - acc: 0.8785 - val_loss: 0.6757 - val_acc: 0.7600\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.3742 - acc: 0.8803 - val_loss: 0.6736 - val_acc: 0.7580\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.3714 - acc: 0.8799 - val_loss: 0.6780 - val_acc: 0.7530\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.3686 - acc: 0.8816 - val_loss: 0.6766 - val_acc: 0.7550\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.3658 - acc: 0.8825 - val_loss: 0.6770 - val_acc: 0.7560\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.3637 - acc: 0.8815 - val_loss: 0.6757 - val_acc: 0.7580\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.3605 - acc: 0.8844 - val_loss: 0.6764 - val_acc: 0.7580\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.3581 - acc: 0.8841 - val_loss: 0.6765 - val_acc: 0.7540\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.3552 - acc: 0.8863 - val_loss: 0.6802 - val_acc: 0.7590\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.3536 - acc: 0.8872 - val_loss: 0.6773 - val_acc: 0.7580\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.3507 - acc: 0.8864 - val_loss: 0.6788 - val_acc: 0.7530\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.3483 - acc: 0.8888 - val_loss: 0.6781 - val_acc: 0.7550\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.3457 - acc: 0.8865 - val_loss: 0.6796 - val_acc: 0.7620\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.3433 - acc: 0.8907 - val_loss: 0.6801 - val_acc: 0.7600\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.3407 - acc: 0.8887 - val_loss: 0.6799 - val_acc: 0.7580\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.3384 - acc: 0.8925 - val_loss: 0.6799 - val_acc: 0.7630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.3358 - acc: 0.8913 - val_loss: 0.6831 - val_acc: 0.7570\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 31us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 35us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3336000207980474, 0.8934666666666666]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6161372900009155, 0.7786666661898295]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! We remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.9543 - acc: 0.1576 - val_loss: 1.9389 - val_acc: 0.1730\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.9324 - acc: 0.1836 - val_loss: 1.9232 - val_acc: 0.1980\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.9171 - acc: 0.2059 - val_loss: 1.9095 - val_acc: 0.1960\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.9016 - acc: 0.2187 - val_loss: 1.8953 - val_acc: 0.2030\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.8850 - acc: 0.2260 - val_loss: 1.8798 - val_acc: 0.2100\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.8668 - acc: 0.2319 - val_loss: 1.8622 - val_acc: 0.2120\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.8463 - acc: 0.2411 - val_loss: 1.8416 - val_acc: 0.2210\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.8222 - acc: 0.2543 - val_loss: 1.8170 - val_acc: 0.2510\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.7933 - acc: 0.2763 - val_loss: 1.7875 - val_acc: 0.2750\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.7586 - acc: 0.3009 - val_loss: 1.7518 - val_acc: 0.3090\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.7174 - acc: 0.3359 - val_loss: 1.7087 - val_acc: 0.3690\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.6692 - acc: 0.3885 - val_loss: 1.6600 - val_acc: 0.3990\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.6160 - acc: 0.4235 - val_loss: 1.6061 - val_acc: 0.4490\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5589 - acc: 0.4597 - val_loss: 1.5491 - val_acc: 0.4860\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4988 - acc: 0.5013 - val_loss: 1.4903 - val_acc: 0.5160\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4379 - acc: 0.5337 - val_loss: 1.4313 - val_acc: 0.5460\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3773 - acc: 0.5693 - val_loss: 1.3738 - val_acc: 0.5690\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3189 - acc: 0.5939 - val_loss: 1.3200 - val_acc: 0.5910\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.2637 - acc: 0.6177 - val_loss: 1.2693 - val_acc: 0.6080\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.2119 - acc: 0.6380 - val_loss: 1.2233 - val_acc: 0.6210\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.1639 - acc: 0.6501 - val_loss: 1.1790 - val_acc: 0.6390\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1188 - acc: 0.6649 - val_loss: 1.1393 - val_acc: 0.6480\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0775 - acc: 0.6716 - val_loss: 1.1043 - val_acc: 0.6620\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0397 - acc: 0.6813 - val_loss: 1.0705 - val_acc: 0.6660\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0046 - acc: 0.6905 - val_loss: 1.0401 - val_acc: 0.6670\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9721 - acc: 0.6972 - val_loss: 1.0123 - val_acc: 0.6710\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9425 - acc: 0.7028 - val_loss: 0.9878 - val_acc: 0.6790\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9154 - acc: 0.7077 - val_loss: 0.9631 - val_acc: 0.6840\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8903 - acc: 0.7141 - val_loss: 0.9427 - val_acc: 0.6920\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8667 - acc: 0.7192 - val_loss: 0.9239 - val_acc: 0.6980\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8457 - acc: 0.7251 - val_loss: 0.9070 - val_acc: 0.7000\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8264 - acc: 0.7263 - val_loss: 0.8913 - val_acc: 0.7020\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8075 - acc: 0.7315 - val_loss: 0.8773 - val_acc: 0.6980\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.7905 - acc: 0.7357 - val_loss: 0.8625 - val_acc: 0.7150\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.7746 - acc: 0.7383 - val_loss: 0.8504 - val_acc: 0.7130\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.7597 - acc: 0.7423 - val_loss: 0.8405 - val_acc: 0.7060\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.7458 - acc: 0.7443 - val_loss: 0.8278 - val_acc: 0.7090\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.7326 - acc: 0.7484 - val_loss: 0.8193 - val_acc: 0.7130\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.7204 - acc: 0.7549 - val_loss: 0.8100 - val_acc: 0.7190\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.7085 - acc: 0.7579 - val_loss: 0.8018 - val_acc: 0.7250\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.6976 - acc: 0.7597 - val_loss: 0.7952 - val_acc: 0.7200\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.6872 - acc: 0.7628 - val_loss: 0.7877 - val_acc: 0.7240\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.6771 - acc: 0.7641 - val_loss: 0.7795 - val_acc: 0.7310\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.6676 - acc: 0.7685 - val_loss: 0.7746 - val_acc: 0.7330\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.6587 - acc: 0.7709 - val_loss: 0.7684 - val_acc: 0.7270\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.6501 - acc: 0.7699 - val_loss: 0.7628 - val_acc: 0.7250\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.6418 - acc: 0.7756 - val_loss: 0.7571 - val_acc: 0.7350\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.6343 - acc: 0.7765 - val_loss: 0.7537 - val_acc: 0.7290\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.6264 - acc: 0.7772 - val_loss: 0.7482 - val_acc: 0.7320\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.6193 - acc: 0.7808 - val_loss: 0.7428 - val_acc: 0.7350\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.6120 - acc: 0.7796 - val_loss: 0.7396 - val_acc: 0.7370\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.6056 - acc: 0.7831 - val_loss: 0.7371 - val_acc: 0.7380\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.5989 - acc: 0.7847 - val_loss: 0.7325 - val_acc: 0.7340\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.5928 - acc: 0.7865 - val_loss: 0.7292 - val_acc: 0.7350\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.5862 - acc: 0.7896 - val_loss: 0.7282 - val_acc: 0.7410\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.5806 - acc: 0.7920 - val_loss: 0.7270 - val_acc: 0.7370\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.5749 - acc: 0.7928 - val_loss: 0.7215 - val_acc: 0.7350\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.5690 - acc: 0.7943 - val_loss: 0.7187 - val_acc: 0.7410\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.5637 - acc: 0.7960 - val_loss: 0.7170 - val_acc: 0.7420\n",
      "Epoch 60/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.5582 - acc: 0.7984 - val_loss: 0.7141 - val_acc: 0.7400\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 38us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 41us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5547114474137624, 0.7958666666984558]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6799592917760213, 0.746666666507721]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 2.6072 - acc: 0.1480 - val_loss: 2.5839 - val_acc: 0.1720\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 2.5745 - acc: 0.1956 - val_loss: 2.5602 - val_acc: 0.2150\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 2.5473 - acc: 0.2340 - val_loss: 2.5349 - val_acc: 0.2390\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 2.5184 - acc: 0.2575 - val_loss: 2.5070 - val_acc: 0.2640\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 2.4867 - acc: 0.2728 - val_loss: 2.4764 - val_acc: 0.2620\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 2.4518 - acc: 0.2892 - val_loss: 2.4428 - val_acc: 0.2770\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 2.4144 - acc: 0.3087 - val_loss: 2.4058 - val_acc: 0.2970\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 2.3740 - acc: 0.3263 - val_loss: 2.3654 - val_acc: 0.3220\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 2.3311 - acc: 0.3461 - val_loss: 2.3233 - val_acc: 0.3440\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 2.2873 - acc: 0.3684 - val_loss: 2.2811 - val_acc: 0.3600\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 2.2429 - acc: 0.3952 - val_loss: 2.2394 - val_acc: 0.3980\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 2.1994 - acc: 0.4241 - val_loss: 2.1980 - val_acc: 0.4340\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 2.1563 - acc: 0.4539 - val_loss: 2.1582 - val_acc: 0.4520\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 2.1138 - acc: 0.4832 - val_loss: 2.1170 - val_acc: 0.4810\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 2.0716 - acc: 0.5057 - val_loss: 2.0777 - val_acc: 0.4970\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 2.0297 - acc: 0.5304 - val_loss: 2.0401 - val_acc: 0.5220\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.9885 - acc: 0.5521 - val_loss: 2.0008 - val_acc: 0.5490\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.9478 - acc: 0.5707 - val_loss: 1.9627 - val_acc: 0.5680\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.9079 - acc: 0.5875 - val_loss: 1.9265 - val_acc: 0.5830\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.8693 - acc: 0.6044 - val_loss: 1.8928 - val_acc: 0.5870\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.8320 - acc: 0.6184 - val_loss: 1.8566 - val_acc: 0.5980\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.7953 - acc: 0.6287 - val_loss: 1.8229 - val_acc: 0.6290\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.7598 - acc: 0.6401 - val_loss: 1.7924 - val_acc: 0.6210\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.7260 - acc: 0.6499 - val_loss: 1.7592 - val_acc: 0.6350\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.6931 - acc: 0.6603 - val_loss: 1.7304 - val_acc: 0.6450\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.6618 - acc: 0.6691 - val_loss: 1.7029 - val_acc: 0.6570\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.6320 - acc: 0.6767 - val_loss: 1.6745 - val_acc: 0.6620\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.6031 - acc: 0.6853 - val_loss: 1.6514 - val_acc: 0.6720\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.5763 - acc: 0.6937 - val_loss: 1.6281 - val_acc: 0.6760\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.5504 - acc: 0.6980 - val_loss: 1.6055 - val_acc: 0.6730\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.5263 - acc: 0.7047 - val_loss: 1.5863 - val_acc: 0.6840\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.5033 - acc: 0.7104 - val_loss: 1.5669 - val_acc: 0.6920\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.4816 - acc: 0.7157 - val_loss: 1.5445 - val_acc: 0.6940\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.4606 - acc: 0.7196 - val_loss: 1.5278 - val_acc: 0.7020\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.4410 - acc: 0.7264 - val_loss: 1.5128 - val_acc: 0.7020\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.4227 - acc: 0.7299 - val_loss: 1.4965 - val_acc: 0.7060\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.4051 - acc: 0.7328 - val_loss: 1.4824 - val_acc: 0.7020\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3887 - acc: 0.7384 - val_loss: 1.4683 - val_acc: 0.7060\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3725 - acc: 0.7412 - val_loss: 1.4545 - val_acc: 0.7140\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3579 - acc: 0.7441 - val_loss: 1.4432 - val_acc: 0.7120\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3436 - acc: 0.7468 - val_loss: 1.4330 - val_acc: 0.7180\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.3302 - acc: 0.7517 - val_loss: 1.4216 - val_acc: 0.7170\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3168 - acc: 0.7523 - val_loss: 1.4114 - val_acc: 0.7150\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3044 - acc: 0.7553 - val_loss: 1.4012 - val_acc: 0.7230\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.2924 - acc: 0.7568 - val_loss: 1.3909 - val_acc: 0.7230\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.2812 - acc: 0.7597 - val_loss: 1.3826 - val_acc: 0.7260\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.2701 - acc: 0.7584 - val_loss: 1.3757 - val_acc: 0.7210\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.2597 - acc: 0.7640 - val_loss: 1.3663 - val_acc: 0.7220\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.2495 - acc: 0.7644 - val_loss: 1.3602 - val_acc: 0.7280\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.2395 - acc: 0.7689 - val_loss: 1.3515 - val_acc: 0.7180\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.2301 - acc: 0.7685 - val_loss: 1.3445 - val_acc: 0.7220\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.2208 - acc: 0.7732 - val_loss: 1.3370 - val_acc: 0.7240\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.2114 - acc: 0.7757 - val_loss: 1.3306 - val_acc: 0.7300\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.2033 - acc: 0.7741 - val_loss: 1.3254 - val_acc: 0.7300\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1945 - acc: 0.7784 - val_loss: 1.3181 - val_acc: 0.7250\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1865 - acc: 0.7771 - val_loss: 1.3132 - val_acc: 0.7320\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.1780 - acc: 0.7820 - val_loss: 1.3091 - val_acc: 0.7340\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1710 - acc: 0.7847 - val_loss: 1.3011 - val_acc: 0.7270\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.1626 - acc: 0.7855 - val_loss: 1.2968 - val_acc: 0.7360\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1554 - acc: 0.7891 - val_loss: 1.2919 - val_acc: 0.7400\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1483 - acc: 0.7867 - val_loss: 1.2869 - val_acc: 0.7300\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1412 - acc: 0.7909 - val_loss: 1.2806 - val_acc: 0.7380\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.1340 - acc: 0.7921 - val_loss: 1.2753 - val_acc: 0.7380\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.1275 - acc: 0.7927 - val_loss: 1.2709 - val_acc: 0.7390\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.1206 - acc: 0.7952 - val_loss: 1.2663 - val_acc: 0.7390\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.1136 - acc: 0.7967 - val_loss: 1.2669 - val_acc: 0.7420\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.1082 - acc: 0.7955 - val_loss: 1.2592 - val_acc: 0.7470\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1016 - acc: 0.8003 - val_loss: 1.2540 - val_acc: 0.7430\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0952 - acc: 0.7980 - val_loss: 1.2503 - val_acc: 0.7390\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0896 - acc: 0.8027 - val_loss: 1.2471 - val_acc: 0.7420\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0833 - acc: 0.8023 - val_loss: 1.2450 - val_acc: 0.7410\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0773 - acc: 0.8048 - val_loss: 1.2398 - val_acc: 0.7430\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0717 - acc: 0.8059 - val_loss: 1.2366 - val_acc: 0.7450\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0664 - acc: 0.8047 - val_loss: 1.2322 - val_acc: 0.7440\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0606 - acc: 0.8073 - val_loss: 1.2294 - val_acc: 0.7430\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0553 - acc: 0.8088 - val_loss: 1.2245 - val_acc: 0.7460\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0498 - acc: 0.8105 - val_loss: 1.2231 - val_acc: 0.7430\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0442 - acc: 0.8115 - val_loss: 1.2181 - val_acc: 0.7450\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0395 - acc: 0.8119 - val_loss: 1.2144 - val_acc: 0.7470\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0344 - acc: 0.8143 - val_loss: 1.2115 - val_acc: 0.7490\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0293 - acc: 0.8144 - val_loss: 1.2086 - val_acc: 0.7500\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0242 - acc: 0.8172 - val_loss: 1.2057 - val_acc: 0.7470\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0192 - acc: 0.8172 - val_loss: 1.2026 - val_acc: 0.7480\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0142 - acc: 0.8195 - val_loss: 1.2025 - val_acc: 0.7490\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0098 - acc: 0.8207 - val_loss: 1.1966 - val_acc: 0.7490\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0044 - acc: 0.8204 - val_loss: 1.1976 - val_acc: 0.7460\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0000 - acc: 0.8217 - val_loss: 1.1927 - val_acc: 0.7450\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9957 - acc: 0.8241 - val_loss: 1.1886 - val_acc: 0.7510\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9910 - acc: 0.8244 - val_loss: 1.1876 - val_acc: 0.7510\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9865 - acc: 0.8263 - val_loss: 1.1848 - val_acc: 0.7530\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9821 - acc: 0.8256 - val_loss: 1.1802 - val_acc: 0.7530\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9776 - acc: 0.8269 - val_loss: 1.1778 - val_acc: 0.7540\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9733 - acc: 0.8295 - val_loss: 1.1770 - val_acc: 0.7490\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9692 - acc: 0.8299 - val_loss: 1.1721 - val_acc: 0.7530\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9648 - acc: 0.8304 - val_loss: 1.1699 - val_acc: 0.7560\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9604 - acc: 0.8328 - val_loss: 1.1683 - val_acc: 0.7500\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9563 - acc: 0.8335 - val_loss: 1.1659 - val_acc: 0.7520\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9520 - acc: 0.8371 - val_loss: 1.1640 - val_acc: 0.7530\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9481 - acc: 0.8364 - val_loss: 1.1611 - val_acc: 0.7580\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9435 - acc: 0.8361 - val_loss: 1.1596 - val_acc: 0.7540\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9400 - acc: 0.8388 - val_loss: 1.1569 - val_acc: 0.7590\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9360 - acc: 0.8392 - val_loss: 1.1553 - val_acc: 0.7600\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9320 - acc: 0.8392 - val_loss: 1.1519 - val_acc: 0.7570\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9283 - acc: 0.8405 - val_loss: 1.1501 - val_acc: 0.7560\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9242 - acc: 0.8423 - val_loss: 1.1480 - val_acc: 0.7560\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9204 - acc: 0.8431 - val_loss: 1.1465 - val_acc: 0.7570\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9168 - acc: 0.8436 - val_loss: 1.1463 - val_acc: 0.7590\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9131 - acc: 0.8444 - val_loss: 1.1413 - val_acc: 0.7540\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9094 - acc: 0.8473 - val_loss: 1.1410 - val_acc: 0.7560\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9061 - acc: 0.8479 - val_loss: 1.1404 - val_acc: 0.7600\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9022 - acc: 0.8485 - val_loss: 1.1369 - val_acc: 0.7600\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8987 - acc: 0.8500 - val_loss: 1.1348 - val_acc: 0.7580\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8950 - acc: 0.8504 - val_loss: 1.1344 - val_acc: 0.7610\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8914 - acc: 0.8517 - val_loss: 1.1310 - val_acc: 0.7590\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8878 - acc: 0.8507 - val_loss: 1.1284 - val_acc: 0.7590\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8844 - acc: 0.8535 - val_loss: 1.1285 - val_acc: 0.7550\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8813 - acc: 0.8545 - val_loss: 1.1246 - val_acc: 0.7600\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8778 - acc: 0.8545 - val_loss: 1.1246 - val_acc: 0.7550\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8746 - acc: 0.8560 - val_loss: 1.1219 - val_acc: 0.7600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8712 - acc: 0.8540 - val_loss: 1.1192 - val_acc: 0.7620\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 16.0095 - acc: 0.1633 - val_loss: 15.6045 - val_acc: 0.1790\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 15.2471 - acc: 0.2015 - val_loss: 14.8580 - val_acc: 0.2100\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 14.5118 - acc: 0.2189 - val_loss: 14.1359 - val_acc: 0.2270\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 13.7985 - acc: 0.2345 - val_loss: 13.4344 - val_acc: 0.2380\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 13.1048 - acc: 0.2452 - val_loss: 12.7529 - val_acc: 0.2450\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 12.4304 - acc: 0.2595 - val_loss: 12.0907 - val_acc: 0.2570\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 11.7747 - acc: 0.2703 - val_loss: 11.4463 - val_acc: 0.2690\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 11.1378 - acc: 0.2791 - val_loss: 10.8216 - val_acc: 0.2750\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 10.5214 - acc: 0.2887 - val_loss: 10.2171 - val_acc: 0.2870\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 9.9248 - acc: 0.3072 - val_loss: 9.6322 - val_acc: 0.3040\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 9.3477 - acc: 0.3231 - val_loss: 9.0677 - val_acc: 0.3130\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 8.7916 - acc: 0.3503 - val_loss: 8.5243 - val_acc: 0.3390\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 8.2575 - acc: 0.3735 - val_loss: 8.0014 - val_acc: 0.3860\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 7.7448 - acc: 0.4136 - val_loss: 7.5016 - val_acc: 0.4000\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 7.2550 - acc: 0.4399 - val_loss: 7.0246 - val_acc: 0.4310\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 6.7880 - acc: 0.4699 - val_loss: 6.5703 - val_acc: 0.4650\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 6.3444 - acc: 0.4977 - val_loss: 6.1409 - val_acc: 0.4980\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 5.9245 - acc: 0.5263 - val_loss: 5.7347 - val_acc: 0.5030\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 5.5274 - acc: 0.5404 - val_loss: 5.3498 - val_acc: 0.5490\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 5.1530 - acc: 0.5649 - val_loss: 4.9882 - val_acc: 0.5720\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 4.8010 - acc: 0.5859 - val_loss: 4.6491 - val_acc: 0.5920\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 4.4715 - acc: 0.6032 - val_loss: 4.3323 - val_acc: 0.5930\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 4.1643 - acc: 0.6115 - val_loss: 4.0381 - val_acc: 0.5990\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 3.8801 - acc: 0.6216 - val_loss: 3.7674 - val_acc: 0.6230\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 3.6188 - acc: 0.6333 - val_loss: 3.5171 - val_acc: 0.6260\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 3.3798 - acc: 0.6423 - val_loss: 3.2907 - val_acc: 0.6250\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 3.1632 - acc: 0.6460 - val_loss: 3.0857 - val_acc: 0.6370\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 2.9683 - acc: 0.6523 - val_loss: 2.9038 - val_acc: 0.6360\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 2.7947 - acc: 0.6541 - val_loss: 2.7402 - val_acc: 0.6430\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 2.6418 - acc: 0.6588 - val_loss: 2.5994 - val_acc: 0.6420\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 2.5095 - acc: 0.6617 - val_loss: 2.4777 - val_acc: 0.6500\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 2.3964 - acc: 0.6643 - val_loss: 2.3747 - val_acc: 0.6510\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 2.3026 - acc: 0.6639 - val_loss: 2.2897 - val_acc: 0.6490\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 2.2259 - acc: 0.6640 - val_loss: 2.2237 - val_acc: 0.6600\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 2.1668 - acc: 0.6673 - val_loss: 2.1731 - val_acc: 0.6610\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 2.1226 - acc: 0.6672 - val_loss: 2.1350 - val_acc: 0.6580\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 2.0896 - acc: 0.6667 - val_loss: 2.1057 - val_acc: 0.6590\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 2.0626 - acc: 0.6712 - val_loss: 2.0835 - val_acc: 0.6590\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 2.0397 - acc: 0.6705 - val_loss: 2.0633 - val_acc: 0.6660\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 2.0182 - acc: 0.6713 - val_loss: 2.0388 - val_acc: 0.6550\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.9987 - acc: 0.6727 - val_loss: 2.0209 - val_acc: 0.6620\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.9806 - acc: 0.6740 - val_loss: 2.0031 - val_acc: 0.6580\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.9625 - acc: 0.6748 - val_loss: 1.9890 - val_acc: 0.6620\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.9463 - acc: 0.6753 - val_loss: 1.9708 - val_acc: 0.6690\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.9302 - acc: 0.6785 - val_loss: 1.9552 - val_acc: 0.6600\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.9154 - acc: 0.6787 - val_loss: 1.9422 - val_acc: 0.6680\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.9007 - acc: 0.6796 - val_loss: 1.9298 - val_acc: 0.6690\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.8863 - acc: 0.6799 - val_loss: 1.9141 - val_acc: 0.6620\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.8725 - acc: 0.6815 - val_loss: 1.9019 - val_acc: 0.6720\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.8593 - acc: 0.6833 - val_loss: 1.8882 - val_acc: 0.6750\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.8465 - acc: 0.6855 - val_loss: 1.8778 - val_acc: 0.6740\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.8342 - acc: 0.6852 - val_loss: 1.8645 - val_acc: 0.6700\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.8223 - acc: 0.6847 - val_loss: 1.8523 - val_acc: 0.6780\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.8109 - acc: 0.6845 - val_loss: 1.8429 - val_acc: 0.6790\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.7993 - acc: 0.6873 - val_loss: 1.8317 - val_acc: 0.6790\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.7880 - acc: 0.6867 - val_loss: 1.8223 - val_acc: 0.6810\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.7773 - acc: 0.6885 - val_loss: 1.8096 - val_acc: 0.6750\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.7663 - acc: 0.6895 - val_loss: 1.8003 - val_acc: 0.6810\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.7561 - acc: 0.6911 - val_loss: 1.7896 - val_acc: 0.6830\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.7460 - acc: 0.6913 - val_loss: 1.7791 - val_acc: 0.6800\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.7356 - acc: 0.6912 - val_loss: 1.7713 - val_acc: 0.6800\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.7260 - acc: 0.6916 - val_loss: 1.7607 - val_acc: 0.6830\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.7163 - acc: 0.6932 - val_loss: 1.7518 - val_acc: 0.6750\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.7068 - acc: 0.6931 - val_loss: 1.7440 - val_acc: 0.6760\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.6973 - acc: 0.6945 - val_loss: 1.7364 - val_acc: 0.6820\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.6892 - acc: 0.6953 - val_loss: 1.7244 - val_acc: 0.6850\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.6792 - acc: 0.6952 - val_loss: 1.7165 - val_acc: 0.6900\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.6708 - acc: 0.6956 - val_loss: 1.7117 - val_acc: 0.6880\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.6625 - acc: 0.6959 - val_loss: 1.6986 - val_acc: 0.6820\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.6539 - acc: 0.6955 - val_loss: 1.6942 - val_acc: 0.6900\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.6455 - acc: 0.6973 - val_loss: 1.6842 - val_acc: 0.6870\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.6370 - acc: 0.6976 - val_loss: 1.6759 - val_acc: 0.6870\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.6293 - acc: 0.6985 - val_loss: 1.6671 - val_acc: 0.6910\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.6210 - acc: 0.6999 - val_loss: 1.6577 - val_acc: 0.6920\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.6131 - acc: 0.6991 - val_loss: 1.6515 - val_acc: 0.6930\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.6053 - acc: 0.7000 - val_loss: 1.6438 - val_acc: 0.6970\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5982 - acc: 0.6992 - val_loss: 1.6358 - val_acc: 0.6960\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5902 - acc: 0.6999 - val_loss: 1.6334 - val_acc: 0.6940\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5831 - acc: 0.7017 - val_loss: 1.6237 - val_acc: 0.6930\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5756 - acc: 0.7015 - val_loss: 1.6173 - val_acc: 0.6950\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.5686 - acc: 0.7025 - val_loss: 1.6095 - val_acc: 0.6940\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5616 - acc: 0.7021 - val_loss: 1.6012 - val_acc: 0.7030\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.5541 - acc: 0.7024 - val_loss: 1.5950 - val_acc: 0.6990\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.5478 - acc: 0.7045 - val_loss: 1.5878 - val_acc: 0.7000\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5399 - acc: 0.7051 - val_loss: 1.5811 - val_acc: 0.6960\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5333 - acc: 0.7039 - val_loss: 1.5736 - val_acc: 0.7010\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.5264 - acc: 0.7043 - val_loss: 1.5664 - val_acc: 0.7000\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5194 - acc: 0.7049 - val_loss: 1.5617 - val_acc: 0.7010\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5137 - acc: 0.7068 - val_loss: 1.5568 - val_acc: 0.6970\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5069 - acc: 0.7068 - val_loss: 1.5477 - val_acc: 0.7020\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5001 - acc: 0.7069 - val_loss: 1.5418 - val_acc: 0.7030\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4932 - acc: 0.7083 - val_loss: 1.5376 - val_acc: 0.7000\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4874 - acc: 0.7063 - val_loss: 1.5293 - val_acc: 0.7050\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4810 - acc: 0.7092 - val_loss: 1.5224 - val_acc: 0.7020\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4748 - acc: 0.7095 - val_loss: 1.5189 - val_acc: 0.7020\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4684 - acc: 0.7113 - val_loss: 1.5120 - val_acc: 0.7040\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4628 - acc: 0.7117 - val_loss: 1.5109 - val_acc: 0.6960\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4572 - acc: 0.7108 - val_loss: 1.5039 - val_acc: 0.7040\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4509 - acc: 0.7104 - val_loss: 1.4977 - val_acc: 0.6990\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4448 - acc: 0.7121 - val_loss: 1.4887 - val_acc: 0.7010\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4390 - acc: 0.7125 - val_loss: 1.4817 - val_acc: 0.7020\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4332 - acc: 0.7132 - val_loss: 1.4749 - val_acc: 0.7010\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4274 - acc: 0.7121 - val_loss: 1.4754 - val_acc: 0.7000\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4224 - acc: 0.7128 - val_loss: 1.4661 - val_acc: 0.7060\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4167 - acc: 0.7136 - val_loss: 1.4652 - val_acc: 0.7070\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4115 - acc: 0.7155 - val_loss: 1.4546 - val_acc: 0.7080\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4062 - acc: 0.7159 - val_loss: 1.4525 - val_acc: 0.7060\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4006 - acc: 0.7175 - val_loss: 1.4447 - val_acc: 0.7050\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.3947 - acc: 0.7177 - val_loss: 1.4396 - val_acc: 0.7030\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3897 - acc: 0.7175 - val_loss: 1.4375 - val_acc: 0.7080\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3849 - acc: 0.7185 - val_loss: 1.4309 - val_acc: 0.7090\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3792 - acc: 0.7189 - val_loss: 1.4238 - val_acc: 0.7080\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3748 - acc: 0.7197 - val_loss: 1.4181 - val_acc: 0.7070\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3695 - acc: 0.7191 - val_loss: 1.4120 - val_acc: 0.7080\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3650 - acc: 0.7200 - val_loss: 1.4080 - val_acc: 0.7090\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3593 - acc: 0.7209 - val_loss: 1.4045 - val_acc: 0.7080\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3545 - acc: 0.7211 - val_loss: 1.3974 - val_acc: 0.7110\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3500 - acc: 0.7203 - val_loss: 1.3953 - val_acc: 0.7120\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3449 - acc: 0.7224 - val_loss: 1.3905 - val_acc: 0.7100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3406 - acc: 0.7211 - val_loss: 1.3863 - val_acc: 0.7120\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 16.0140 - acc: 0.1877 - val_loss: 15.6118 - val_acc: 0.2100\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 15.2533 - acc: 0.2067 - val_loss: 14.8668 - val_acc: 0.2240\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 14.5171 - acc: 0.2219 - val_loss: 14.1433 - val_acc: 0.2290\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 13.8011 - acc: 0.2360 - val_loss: 13.4388 - val_acc: 0.2440\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 13.1038 - acc: 0.2528 - val_loss: 12.7534 - val_acc: 0.2580\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 12.4253 - acc: 0.2772 - val_loss: 12.0869 - val_acc: 0.2720\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 11.7658 - acc: 0.3027 - val_loss: 11.4387 - val_acc: 0.3140\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 11.1259 - acc: 0.3431 - val_loss: 10.8103 - val_acc: 0.3470\n",
      "Epoch 9/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 10.5062 - acc: 0.3781 - val_loss: 10.2028 - val_acc: 0.3780\n",
      "Epoch 10/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 9.9068 - acc: 0.4124 - val_loss: 9.6137 - val_acc: 0.3970\n",
      "Epoch 11/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 9.3279 - acc: 0.4441 - val_loss: 9.0463 - val_acc: 0.4310\n",
      "Epoch 12/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 8.7708 - acc: 0.4668 - val_loss: 8.5007 - val_acc: 0.4610\n",
      "Epoch 13/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 8.2357 - acc: 0.4971 - val_loss: 7.9774 - val_acc: 0.4730\n",
      "Epoch 14/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 7.7237 - acc: 0.5213 - val_loss: 7.4776 - val_acc: 0.4990\n",
      "Epoch 15/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 7.2345 - acc: 0.5405 - val_loss: 7.0011 - val_acc: 0.5270\n",
      "Epoch 16/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 6.7686 - acc: 0.5620 - val_loss: 6.5474 - val_acc: 0.5490\n",
      "Epoch 17/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 6.3256 - acc: 0.5787 - val_loss: 6.1182 - val_acc: 0.5860\n",
      "Epoch 18/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 5.9053 - acc: 0.5941 - val_loss: 5.7067 - val_acc: 0.5940\n",
      "Epoch 19/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 5.5074 - acc: 0.6119 - val_loss: 5.3218 - val_acc: 0.5940\n",
      "Epoch 20/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 5.1324 - acc: 0.6183 - val_loss: 4.9565 - val_acc: 0.6200\n",
      "Epoch 21/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 4.7803 - acc: 0.6311 - val_loss: 4.6160 - val_acc: 0.6160\n",
      "Epoch 22/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 4.4521 - acc: 0.6364 - val_loss: 4.2978 - val_acc: 0.6320\n",
      "Epoch 23/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 4.1454 - acc: 0.6468 - val_loss: 4.0029 - val_acc: 0.6420\n",
      "Epoch 24/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 3.8615 - acc: 0.6531 - val_loss: 3.7284 - val_acc: 0.6590\n",
      "Epoch 25/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 3.5996 - acc: 0.6593 - val_loss: 3.4779 - val_acc: 0.6590\n",
      "Epoch 26/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 3.3602 - acc: 0.6624 - val_loss: 3.2500 - val_acc: 0.6740\n",
      "Epoch 27/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 3.1425 - acc: 0.6635 - val_loss: 3.0417 - val_acc: 0.6780\n",
      "Epoch 28/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.9463 - acc: 0.6691 - val_loss: 2.8538 - val_acc: 0.6790\n",
      "Epoch 29/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.7710 - acc: 0.6711 - val_loss: 2.6896 - val_acc: 0.6810\n",
      "Epoch 30/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.6167 - acc: 0.6717 - val_loss: 2.5434 - val_acc: 0.6810\n",
      "Epoch 31/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.4825 - acc: 0.6703 - val_loss: 2.4194 - val_acc: 0.6810\n",
      "Epoch 32/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.3687 - acc: 0.6731 - val_loss: 2.3143 - val_acc: 0.6780\n",
      "Epoch 33/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.2740 - acc: 0.6723 - val_loss: 2.2270 - val_acc: 0.6820\n",
      "Epoch 34/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.1980 - acc: 0.6713 - val_loss: 2.1602 - val_acc: 0.6810\n",
      "Epoch 35/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.1386 - acc: 0.6725 - val_loss: 2.1087 - val_acc: 0.6800\n",
      "Epoch 36/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.0942 - acc: 0.6736 - val_loss: 2.0704 - val_acc: 0.6760\n",
      "Epoch 37/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.0616 - acc: 0.6712 - val_loss: 2.0393 - val_acc: 0.6720\n",
      "Epoch 38/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.0343 - acc: 0.6735 - val_loss: 2.0143 - val_acc: 0.6710\n",
      "Epoch 39/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.0114 - acc: 0.6720 - val_loss: 1.9902 - val_acc: 0.6790\n",
      "Epoch 40/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.9903 - acc: 0.6745 - val_loss: 1.9708 - val_acc: 0.6780\n",
      "Epoch 41/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.9712 - acc: 0.6747 - val_loss: 1.9519 - val_acc: 0.6830\n",
      "Epoch 42/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.9536 - acc: 0.6751 - val_loss: 1.9332 - val_acc: 0.6780\n",
      "Epoch 43/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.9369 - acc: 0.6753 - val_loss: 1.9168 - val_acc: 0.6820\n",
      "Epoch 44/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.9213 - acc: 0.6769 - val_loss: 1.9012 - val_acc: 0.6770\n",
      "Epoch 45/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.9061 - acc: 0.6791 - val_loss: 1.8871 - val_acc: 0.6790\n",
      "Epoch 46/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8920 - acc: 0.6776 - val_loss: 1.8731 - val_acc: 0.6830\n",
      "Epoch 47/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8783 - acc: 0.6800 - val_loss: 1.8585 - val_acc: 0.6820\n",
      "Epoch 48/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8650 - acc: 0.6796 - val_loss: 1.8430 - val_acc: 0.6850\n",
      "Epoch 49/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8519 - acc: 0.6815 - val_loss: 1.8301 - val_acc: 0.6900\n",
      "Epoch 50/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8395 - acc: 0.6824 - val_loss: 1.8186 - val_acc: 0.6910\n",
      "Epoch 51/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8277 - acc: 0.6844 - val_loss: 1.8058 - val_acc: 0.6900\n",
      "Epoch 52/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8157 - acc: 0.6853 - val_loss: 1.7949 - val_acc: 0.6930\n",
      "Epoch 53/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.8043 - acc: 0.6844 - val_loss: 1.7821 - val_acc: 0.6950\n",
      "Epoch 54/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7931 - acc: 0.6857 - val_loss: 1.7722 - val_acc: 0.6970\n",
      "Epoch 55/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7828 - acc: 0.6863 - val_loss: 1.7612 - val_acc: 0.6880\n",
      "Epoch 56/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7722 - acc: 0.6876 - val_loss: 1.7494 - val_acc: 0.6940\n",
      "Epoch 57/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7616 - acc: 0.6877 - val_loss: 1.7391 - val_acc: 0.6920\n",
      "Epoch 58/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7517 - acc: 0.6887 - val_loss: 1.7376 - val_acc: 0.6940\n",
      "Epoch 59/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7419 - acc: 0.6892 - val_loss: 1.7217 - val_acc: 0.6940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7319 - acc: 0.6887 - val_loss: 1.7170 - val_acc: 0.6940\n",
      "Epoch 61/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7227 - acc: 0.6892 - val_loss: 1.7054 - val_acc: 0.6970\n",
      "Epoch 62/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7136 - acc: 0.6903 - val_loss: 1.6913 - val_acc: 0.6980\n",
      "Epoch 63/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7041 - acc: 0.6924 - val_loss: 1.6814 - val_acc: 0.6940\n",
      "Epoch 64/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6951 - acc: 0.6928 - val_loss: 1.6735 - val_acc: 0.7040\n",
      "Epoch 65/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6864 - acc: 0.6944 - val_loss: 1.6635 - val_acc: 0.7020\n",
      "Epoch 66/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6778 - acc: 0.6957 - val_loss: 1.6551 - val_acc: 0.7050\n",
      "Epoch 67/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6691 - acc: 0.6960 - val_loss: 1.6475 - val_acc: 0.7070\n",
      "Epoch 68/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6604 - acc: 0.6987 - val_loss: 1.6402 - val_acc: 0.7010\n",
      "Epoch 69/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6518 - acc: 0.6996 - val_loss: 1.6361 - val_acc: 0.7010\n",
      "Epoch 70/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6436 - acc: 0.6987 - val_loss: 1.6220 - val_acc: 0.7000\n",
      "Epoch 71/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6354 - acc: 0.7015 - val_loss: 1.6156 - val_acc: 0.7080\n",
      "Epoch 72/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6274 - acc: 0.7015 - val_loss: 1.6048 - val_acc: 0.7010\n",
      "Epoch 73/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6198 - acc: 0.7019 - val_loss: 1.6047 - val_acc: 0.7080\n",
      "Epoch 74/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6122 - acc: 0.7016 - val_loss: 1.5891 - val_acc: 0.7060\n",
      "Epoch 75/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6044 - acc: 0.7020 - val_loss: 1.5813 - val_acc: 0.7070\n",
      "Epoch 76/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5962 - acc: 0.7048 - val_loss: 1.5755 - val_acc: 0.7060\n",
      "Epoch 77/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5886 - acc: 0.7039 - val_loss: 1.5679 - val_acc: 0.7060\n",
      "Epoch 78/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5816 - acc: 0.7048 - val_loss: 1.5609 - val_acc: 0.7080\n",
      "Epoch 79/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5739 - acc: 0.7051 - val_loss: 1.5524 - val_acc: 0.7100\n",
      "Epoch 80/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5670 - acc: 0.7047 - val_loss: 1.5454 - val_acc: 0.7080\n",
      "Epoch 81/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5596 - acc: 0.7049 - val_loss: 1.5392 - val_acc: 0.7070\n",
      "Epoch 82/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5523 - acc: 0.7080 - val_loss: 1.5329 - val_acc: 0.7170\n",
      "Epoch 83/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5455 - acc: 0.7063 - val_loss: 1.5254 - val_acc: 0.7150\n",
      "Epoch 84/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5384 - acc: 0.7087 - val_loss: 1.5181 - val_acc: 0.7130\n",
      "Epoch 85/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5314 - acc: 0.7088 - val_loss: 1.5132 - val_acc: 0.7110\n",
      "Epoch 86/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5249 - acc: 0.7084 - val_loss: 1.5042 - val_acc: 0.7140\n",
      "Epoch 87/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5176 - acc: 0.7093 - val_loss: 1.5022 - val_acc: 0.7070\n",
      "Epoch 88/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5115 - acc: 0.7108 - val_loss: 1.4923 - val_acc: 0.7130\n",
      "Epoch 89/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5045 - acc: 0.7117 - val_loss: 1.4951 - val_acc: 0.7090\n",
      "Epoch 90/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4983 - acc: 0.7115 - val_loss: 1.4801 - val_acc: 0.7140\n",
      "Epoch 91/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4912 - acc: 0.7127 - val_loss: 1.4728 - val_acc: 0.7140\n",
      "Epoch 92/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4852 - acc: 0.7104 - val_loss: 1.4667 - val_acc: 0.7170\n",
      "Epoch 93/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4784 - acc: 0.7136 - val_loss: 1.4594 - val_acc: 0.7080\n",
      "Epoch 94/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4724 - acc: 0.7137 - val_loss: 1.4551 - val_acc: 0.7140\n",
      "Epoch 95/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4658 - acc: 0.7149 - val_loss: 1.4498 - val_acc: 0.7160\n",
      "Epoch 96/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4602 - acc: 0.7151 - val_loss: 1.4421 - val_acc: 0.7170\n",
      "Epoch 97/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4538 - acc: 0.7163 - val_loss: 1.4346 - val_acc: 0.7170\n",
      "Epoch 98/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4470 - acc: 0.7149 - val_loss: 1.4340 - val_acc: 0.7200\n",
      "Epoch 99/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4414 - acc: 0.7152 - val_loss: 1.4272 - val_acc: 0.7180\n",
      "Epoch 100/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4362 - acc: 0.7188 - val_loss: 1.4224 - val_acc: 0.7150\n",
      "Epoch 101/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4301 - acc: 0.7153 - val_loss: 1.4136 - val_acc: 0.7190\n",
      "Epoch 102/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4246 - acc: 0.7169 - val_loss: 1.4046 - val_acc: 0.7200\n",
      "Epoch 103/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4182 - acc: 0.7196 - val_loss: 1.4068 - val_acc: 0.7190\n",
      "Epoch 104/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4132 - acc: 0.7200 - val_loss: 1.3998 - val_acc: 0.7190\n",
      "Epoch 105/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4072 - acc: 0.7176 - val_loss: 1.3891 - val_acc: 0.7180\n",
      "Epoch 106/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4016 - acc: 0.7204 - val_loss: 1.3859 - val_acc: 0.7220\n",
      "Epoch 107/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3966 - acc: 0.7208 - val_loss: 1.3825 - val_acc: 0.7210\n",
      "Epoch 108/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3906 - acc: 0.7225 - val_loss: 1.3804 - val_acc: 0.7230\n",
      "Epoch 109/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3854 - acc: 0.7219 - val_loss: 1.3692 - val_acc: 0.7220\n",
      "Epoch 110/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3802 - acc: 0.7215 - val_loss: 1.3651 - val_acc: 0.7200\n",
      "Epoch 111/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3750 - acc: 0.7201 - val_loss: 1.3593 - val_acc: 0.7210\n",
      "Epoch 112/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3703 - acc: 0.7241 - val_loss: 1.3563 - val_acc: 0.7220\n",
      "Epoch 113/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3650 - acc: 0.7241 - val_loss: 1.3489 - val_acc: 0.7250\n",
      "Epoch 114/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3596 - acc: 0.7249 - val_loss: 1.3460 - val_acc: 0.7250\n",
      "Epoch 115/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3543 - acc: 0.7255 - val_loss: 1.3375 - val_acc: 0.7250\n",
      "Epoch 116/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3497 - acc: 0.7248 - val_loss: 1.3340 - val_acc: 0.7270\n",
      "Epoch 117/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3450 - acc: 0.7249 - val_loss: 1.3339 - val_acc: 0.7220\n",
      "Epoch 118/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3404 - acc: 0.7247 - val_loss: 1.3267 - val_acc: 0.7210\n",
      "Epoch 119/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3356 - acc: 0.7267 - val_loss: 1.3237 - val_acc: 0.7160\n",
      "Epoch 120/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3309 - acc: 0.7299 - val_loss: 1.3153 - val_acc: 0.7290\n",
      "Epoch 121/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3262 - acc: 0.7273 - val_loss: 1.3085 - val_acc: 0.7270\n",
      "Epoch 122/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3214 - acc: 0.7275 - val_loss: 1.3051 - val_acc: 0.7210\n",
      "Epoch 123/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3167 - acc: 0.7279 - val_loss: 1.3032 - val_acc: 0.7250\n",
      "Epoch 124/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3118 - acc: 0.7288 - val_loss: 1.2985 - val_acc: 0.7240\n",
      "Epoch 125/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3073 - acc: 0.7301 - val_loss: 1.2923 - val_acc: 0.7240\n",
      "Epoch 126/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3030 - acc: 0.7299 - val_loss: 1.2887 - val_acc: 0.7270\n",
      "Epoch 127/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2984 - acc: 0.7301 - val_loss: 1.2880 - val_acc: 0.7230\n",
      "Epoch 128/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2947 - acc: 0.7303 - val_loss: 1.2849 - val_acc: 0.7180\n",
      "Epoch 129/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2899 - acc: 0.7319 - val_loss: 1.2752 - val_acc: 0.7220\n",
      "Epoch 130/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2858 - acc: 0.7301 - val_loss: 1.2707 - val_acc: 0.7260\n",
      "Epoch 131/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2819 - acc: 0.7319 - val_loss: 1.2682 - val_acc: 0.7240\n",
      "Epoch 132/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2771 - acc: 0.7319 - val_loss: 1.2628 - val_acc: 0.7270\n",
      "Epoch 133/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2734 - acc: 0.7331 - val_loss: 1.2590 - val_acc: 0.7290\n",
      "Epoch 134/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2691 - acc: 0.7335 - val_loss: 1.2582 - val_acc: 0.7250\n",
      "Epoch 135/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2655 - acc: 0.7348 - val_loss: 1.2505 - val_acc: 0.7250\n",
      "Epoch 136/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2614 - acc: 0.7351 - val_loss: 1.2478 - val_acc: 0.7320\n",
      "Epoch 137/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2575 - acc: 0.7325 - val_loss: 1.2434 - val_acc: 0.7270\n",
      "Epoch 138/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2538 - acc: 0.7355 - val_loss: 1.2403 - val_acc: 0.7290\n",
      "Epoch 139/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2497 - acc: 0.7355 - val_loss: 1.2362 - val_acc: 0.7290\n",
      "Epoch 140/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2456 - acc: 0.7341 - val_loss: 1.2344 - val_acc: 0.7310\n",
      "Epoch 141/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2418 - acc: 0.7356 - val_loss: 1.2292 - val_acc: 0.7350\n",
      "Epoch 142/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2380 - acc: 0.7364 - val_loss: 1.2267 - val_acc: 0.7320\n",
      "Epoch 143/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2344 - acc: 0.7365 - val_loss: 1.2225 - val_acc: 0.7280\n",
      "Epoch 144/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2306 - acc: 0.7371 - val_loss: 1.2170 - val_acc: 0.7280\n",
      "Epoch 145/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2272 - acc: 0.7349 - val_loss: 1.2162 - val_acc: 0.7320\n",
      "Epoch 146/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2233 - acc: 0.7385 - val_loss: 1.2114 - val_acc: 0.7290\n",
      "Epoch 147/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2199 - acc: 0.7385 - val_loss: 1.2098 - val_acc: 0.7310\n",
      "Epoch 148/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2166 - acc: 0.7379 - val_loss: 1.2034 - val_acc: 0.7340\n",
      "Epoch 149/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2128 - acc: 0.7393 - val_loss: 1.2085 - val_acc: 0.7230\n",
      "Epoch 150/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2096 - acc: 0.7389 - val_loss: 1.1977 - val_acc: 0.7310\n",
      "Epoch 151/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2066 - acc: 0.7391 - val_loss: 1.1958 - val_acc: 0.7320\n",
      "Epoch 152/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2030 - acc: 0.7389 - val_loss: 1.1909 - val_acc: 0.7340\n",
      "Epoch 153/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1992 - acc: 0.7396 - val_loss: 1.1875 - val_acc: 0.7350\n",
      "Epoch 154/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1969 - acc: 0.7380 - val_loss: 1.1904 - val_acc: 0.7290\n",
      "Epoch 155/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1932 - acc: 0.7395 - val_loss: 1.1811 - val_acc: 0.7340\n",
      "Epoch 156/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1896 - acc: 0.7405 - val_loss: 1.1802 - val_acc: 0.7370\n",
      "Epoch 157/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1861 - acc: 0.7408 - val_loss: 1.1754 - val_acc: 0.7300\n",
      "Epoch 158/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1834 - acc: 0.7409 - val_loss: 1.1755 - val_acc: 0.7320\n",
      "Epoch 159/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1803 - acc: 0.7425 - val_loss: 1.1680 - val_acc: 0.7360\n",
      "Epoch 160/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1766 - acc: 0.7424 - val_loss: 1.1751 - val_acc: 0.7280\n",
      "Epoch 161/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1752 - acc: 0.7417 - val_loss: 1.1609 - val_acc: 0.7340\n",
      "Epoch 162/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1716 - acc: 0.7423 - val_loss: 1.1612 - val_acc: 0.7360\n",
      "Epoch 163/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1687 - acc: 0.7409 - val_loss: 1.1594 - val_acc: 0.7370\n",
      "Epoch 164/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1658 - acc: 0.7441 - val_loss: 1.1571 - val_acc: 0.7350\n",
      "Epoch 165/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1629 - acc: 0.7419 - val_loss: 1.1572 - val_acc: 0.7390\n",
      "Epoch 166/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1598 - acc: 0.7429 - val_loss: 1.1513 - val_acc: 0.7430\n",
      "Epoch 167/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1577 - acc: 0.7429 - val_loss: 1.1462 - val_acc: 0.7390\n",
      "Epoch 168/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1546 - acc: 0.7427 - val_loss: 1.1453 - val_acc: 0.7410\n",
      "Epoch 169/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1517 - acc: 0.7431 - val_loss: 1.1406 - val_acc: 0.7410\n",
      "Epoch 170/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1490 - acc: 0.7461 - val_loss: 1.1447 - val_acc: 0.7300\n",
      "Epoch 171/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1470 - acc: 0.7456 - val_loss: 1.1377 - val_acc: 0.7410\n",
      "Epoch 172/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1438 - acc: 0.7455 - val_loss: 1.1356 - val_acc: 0.7440\n",
      "Epoch 173/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1415 - acc: 0.7457 - val_loss: 1.1324 - val_acc: 0.7390\n",
      "Epoch 174/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1394 - acc: 0.7433 - val_loss: 1.1291 - val_acc: 0.7370\n",
      "Epoch 175/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1362 - acc: 0.7455 - val_loss: 1.1279 - val_acc: 0.7440\n",
      "Epoch 176/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1352 - acc: 0.7449 - val_loss: 1.1235 - val_acc: 0.7370\n",
      "Epoch 177/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1320 - acc: 0.7447 - val_loss: 1.1211 - val_acc: 0.7380\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1293 - acc: 0.7463 - val_loss: 1.1236 - val_acc: 0.7380\n",
      "Epoch 179/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1279 - acc: 0.7457 - val_loss: 1.1186 - val_acc: 0.7400\n",
      "Epoch 180/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1250 - acc: 0.7467 - val_loss: 1.1250 - val_acc: 0.7390\n",
      "Epoch 181/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1234 - acc: 0.7436 - val_loss: 1.1210 - val_acc: 0.7360\n",
      "Epoch 182/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1214 - acc: 0.7468 - val_loss: 1.1116 - val_acc: 0.7430\n",
      "Epoch 183/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1188 - acc: 0.7488 - val_loss: 1.1138 - val_acc: 0.7380\n",
      "Epoch 184/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1174 - acc: 0.7473 - val_loss: 1.1087 - val_acc: 0.7440\n",
      "Epoch 185/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1145 - acc: 0.7479 - val_loss: 1.1078 - val_acc: 0.7410\n",
      "Epoch 186/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1129 - acc: 0.7463 - val_loss: 1.1084 - val_acc: 0.7450\n",
      "Epoch 187/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1110 - acc: 0.7476 - val_loss: 1.1017 - val_acc: 0.7440\n",
      "Epoch 188/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1081 - acc: 0.7471 - val_loss: 1.1027 - val_acc: 0.7410\n",
      "Epoch 189/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1065 - acc: 0.7489 - val_loss: 1.0996 - val_acc: 0.7380\n",
      "Epoch 190/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1045 - acc: 0.7473 - val_loss: 1.0977 - val_acc: 0.7430\n",
      "Epoch 191/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1028 - acc: 0.7496 - val_loss: 1.1026 - val_acc: 0.7370\n",
      "Epoch 192/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1010 - acc: 0.7475 - val_loss: 1.0954 - val_acc: 0.7370\n",
      "Epoch 193/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0997 - acc: 0.7484 - val_loss: 1.0904 - val_acc: 0.7450\n",
      "Epoch 194/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0972 - acc: 0.7500 - val_loss: 1.0884 - val_acc: 0.7410\n",
      "Epoch 195/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0953 - acc: 0.7503 - val_loss: 1.0877 - val_acc: 0.7400\n",
      "Epoch 196/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0937 - acc: 0.7480 - val_loss: 1.0867 - val_acc: 0.7390\n",
      "Epoch 197/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0922 - acc: 0.7485 - val_loss: 1.0866 - val_acc: 0.7430\n",
      "Epoch 198/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0896 - acc: 0.7497 - val_loss: 1.0832 - val_acc: 0.7440\n",
      "Epoch 199/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0879 - acc: 0.7495 - val_loss: 1.0827 - val_acc: 0.7440\n",
      "Epoch 200/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0863 - acc: 0.7511 - val_loss: 1.0830 - val_acc: 0.7430\n",
      "Epoch 201/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0848 - acc: 0.7515 - val_loss: 1.0847 - val_acc: 0.7440\n",
      "Epoch 202/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0835 - acc: 0.7508 - val_loss: 1.0759 - val_acc: 0.7420\n",
      "Epoch 203/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0813 - acc: 0.7508 - val_loss: 1.0765 - val_acc: 0.7400\n",
      "Epoch 204/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0800 - acc: 0.7508 - val_loss: 1.0759 - val_acc: 0.7480\n",
      "Epoch 205/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0782 - acc: 0.7513 - val_loss: 1.0711 - val_acc: 0.7430\n",
      "Epoch 206/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0765 - acc: 0.7496 - val_loss: 1.0708 - val_acc: 0.7420\n",
      "Epoch 207/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0753 - acc: 0.7517 - val_loss: 1.0696 - val_acc: 0.7420\n",
      "Epoch 208/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0740 - acc: 0.7512 - val_loss: 1.0685 - val_acc: 0.7400\n",
      "Epoch 209/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0720 - acc: 0.7521 - val_loss: 1.0647 - val_acc: 0.7430\n",
      "Epoch 210/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0696 - acc: 0.7523 - val_loss: 1.0644 - val_acc: 0.7400\n",
      "Epoch 211/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0693 - acc: 0.7537 - val_loss: 1.0667 - val_acc: 0.7420\n",
      "Epoch 212/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0681 - acc: 0.7524 - val_loss: 1.0655 - val_acc: 0.7390\n",
      "Epoch 213/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0656 - acc: 0.7537 - val_loss: 1.0602 - val_acc: 0.7420\n",
      "Epoch 214/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0650 - acc: 0.7543 - val_loss: 1.0602 - val_acc: 0.7420\n",
      "Epoch 215/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0624 - acc: 0.7532 - val_loss: 1.0579 - val_acc: 0.7480\n",
      "Epoch 216/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0619 - acc: 0.7535 - val_loss: 1.0581 - val_acc: 0.7390\n",
      "Epoch 217/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0599 - acc: 0.7533 - val_loss: 1.0526 - val_acc: 0.7450\n",
      "Epoch 218/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0584 - acc: 0.7539 - val_loss: 1.0534 - val_acc: 0.7440\n",
      "Epoch 219/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0572 - acc: 0.7547 - val_loss: 1.0521 - val_acc: 0.7430\n",
      "Epoch 220/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0560 - acc: 0.7552 - val_loss: 1.0489 - val_acc: 0.7490\n",
      "Epoch 221/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0544 - acc: 0.7543 - val_loss: 1.0516 - val_acc: 0.7460\n",
      "Epoch 222/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0530 - acc: 0.7552 - val_loss: 1.0571 - val_acc: 0.7400\n",
      "Epoch 223/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0526 - acc: 0.7544 - val_loss: 1.0492 - val_acc: 0.7450\n",
      "Epoch 224/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0509 - acc: 0.7527 - val_loss: 1.0506 - val_acc: 0.7450\n",
      "Epoch 225/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0493 - acc: 0.7540 - val_loss: 1.0441 - val_acc: 0.7420\n",
      "Epoch 226/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0479 - acc: 0.7555 - val_loss: 1.0413 - val_acc: 0.7430\n",
      "Epoch 227/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0465 - acc: 0.7545 - val_loss: 1.0415 - val_acc: 0.7440\n",
      "Epoch 228/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0449 - acc: 0.7561 - val_loss: 1.0418 - val_acc: 0.7440\n",
      "Epoch 229/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0438 - acc: 0.7551 - val_loss: 1.0408 - val_acc: 0.7460\n",
      "Epoch 230/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0421 - acc: 0.7563 - val_loss: 1.0380 - val_acc: 0.7540\n",
      "Epoch 231/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0410 - acc: 0.7564 - val_loss: 1.0371 - val_acc: 0.7440\n",
      "Epoch 232/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0400 - acc: 0.7569 - val_loss: 1.0419 - val_acc: 0.7370\n",
      "Epoch 233/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0390 - acc: 0.7565 - val_loss: 1.0352 - val_acc: 0.7450\n",
      "Epoch 234/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0372 - acc: 0.7571 - val_loss: 1.0390 - val_acc: 0.7400\n",
      "Epoch 235/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0359 - acc: 0.7575 - val_loss: 1.0348 - val_acc: 0.7490\n",
      "Epoch 236/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0357 - acc: 0.7561 - val_loss: 1.0300 - val_acc: 0.7450\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0334 - acc: 0.7564 - val_loss: 1.0296 - val_acc: 0.7510\n",
      "Epoch 238/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0328 - acc: 0.7564 - val_loss: 1.0358 - val_acc: 0.7510\n",
      "Epoch 239/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0323 - acc: 0.7555 - val_loss: 1.0284 - val_acc: 0.7460\n",
      "Epoch 240/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0306 - acc: 0.7579 - val_loss: 1.0279 - val_acc: 0.7490\n",
      "Epoch 241/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0292 - acc: 0.7588 - val_loss: 1.0257 - val_acc: 0.7470\n",
      "Epoch 242/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0285 - acc: 0.7576 - val_loss: 1.0270 - val_acc: 0.7450\n",
      "Epoch 243/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0269 - acc: 0.7560 - val_loss: 1.0248 - val_acc: 0.7420\n",
      "Epoch 244/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0257 - acc: 0.7575 - val_loss: 1.0254 - val_acc: 0.7450\n",
      "Epoch 245/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0244 - acc: 0.7569 - val_loss: 1.0243 - val_acc: 0.7450\n",
      "Epoch 246/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0233 - acc: 0.7599 - val_loss: 1.0241 - val_acc: 0.7460\n",
      "Epoch 247/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0225 - acc: 0.7581 - val_loss: 1.0215 - val_acc: 0.7440\n",
      "Epoch 248/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0215 - acc: 0.7583 - val_loss: 1.0181 - val_acc: 0.7440\n",
      "Epoch 249/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0202 - acc: 0.7609 - val_loss: 1.0187 - val_acc: 0.7470\n",
      "Epoch 250/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0196 - acc: 0.7559 - val_loss: 1.0175 - val_acc: 0.7430\n",
      "Epoch 251/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0179 - acc: 0.7597 - val_loss: 1.0187 - val_acc: 0.7550\n",
      "Epoch 252/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0173 - acc: 0.7612 - val_loss: 1.0198 - val_acc: 0.7440\n",
      "Epoch 253/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0165 - acc: 0.7596 - val_loss: 1.0164 - val_acc: 0.7520\n",
      "Epoch 254/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0160 - acc: 0.7591 - val_loss: 1.0137 - val_acc: 0.7450\n",
      "Epoch 255/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0143 - acc: 0.7608 - val_loss: 1.0162 - val_acc: 0.7460\n",
      "Epoch 256/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0129 - acc: 0.7603 - val_loss: 1.0109 - val_acc: 0.7510\n",
      "Epoch 257/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0121 - acc: 0.7615 - val_loss: 1.0209 - val_acc: 0.7530\n",
      "Epoch 258/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0118 - acc: 0.7607 - val_loss: 1.0087 - val_acc: 0.7530\n",
      "Epoch 259/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0103 - acc: 0.7604 - val_loss: 1.0091 - val_acc: 0.7490\n",
      "Epoch 260/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0092 - acc: 0.7607 - val_loss: 1.0086 - val_acc: 0.7460\n",
      "Epoch 261/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0081 - acc: 0.7613 - val_loss: 1.0151 - val_acc: 0.7430\n",
      "Epoch 262/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0073 - acc: 0.7589 - val_loss: 1.0057 - val_acc: 0.7450\n",
      "Epoch 263/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0056 - acc: 0.7624 - val_loss: 1.0074 - val_acc: 0.7530\n",
      "Epoch 264/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0054 - acc: 0.7617 - val_loss: 1.0050 - val_acc: 0.7430\n",
      "Epoch 265/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0038 - acc: 0.7629 - val_loss: 1.0060 - val_acc: 0.7520\n",
      "Epoch 266/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0037 - acc: 0.7595 - val_loss: 1.0063 - val_acc: 0.7470\n",
      "Epoch 267/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0023 - acc: 0.7624 - val_loss: 1.0027 - val_acc: 0.7550\n",
      "Epoch 268/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0012 - acc: 0.7624 - val_loss: 1.0099 - val_acc: 0.7470\n",
      "Epoch 269/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0015 - acc: 0.7605 - val_loss: 1.0015 - val_acc: 0.7520\n",
      "Epoch 270/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9996 - acc: 0.7640 - val_loss: 0.9987 - val_acc: 0.7490\n",
      "Epoch 271/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9989 - acc: 0.7629 - val_loss: 0.9976 - val_acc: 0.7500\n",
      "Epoch 272/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9978 - acc: 0.7636 - val_loss: 1.0029 - val_acc: 0.7480\n",
      "Epoch 273/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9971 - acc: 0.7621 - val_loss: 0.9973 - val_acc: 0.7540\n",
      "Epoch 274/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9967 - acc: 0.7612 - val_loss: 0.9938 - val_acc: 0.7510\n",
      "Epoch 275/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9952 - acc: 0.7636 - val_loss: 0.9967 - val_acc: 0.7530\n",
      "Epoch 276/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9945 - acc: 0.7635 - val_loss: 1.0000 - val_acc: 0.7500\n",
      "Epoch 277/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9937 - acc: 0.7629 - val_loss: 0.9950 - val_acc: 0.7470\n",
      "Epoch 278/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9926 - acc: 0.7643 - val_loss: 0.9954 - val_acc: 0.7530\n",
      "Epoch 279/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9927 - acc: 0.7635 - val_loss: 0.9956 - val_acc: 0.7470\n",
      "Epoch 280/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9908 - acc: 0.7643 - val_loss: 0.9973 - val_acc: 0.7450\n",
      "Epoch 281/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9906 - acc: 0.7633 - val_loss: 0.9918 - val_acc: 0.7570\n",
      "Epoch 282/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9895 - acc: 0.7609 - val_loss: 0.9884 - val_acc: 0.7470\n",
      "Epoch 283/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9884 - acc: 0.7655 - val_loss: 0.9925 - val_acc: 0.7500\n",
      "Epoch 284/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9876 - acc: 0.7648 - val_loss: 0.9932 - val_acc: 0.7490\n",
      "Epoch 285/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9868 - acc: 0.7620 - val_loss: 0.9950 - val_acc: 0.7450\n",
      "Epoch 286/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9861 - acc: 0.7667 - val_loss: 0.9908 - val_acc: 0.7520\n",
      "Epoch 287/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9855 - acc: 0.7635 - val_loss: 0.9861 - val_acc: 0.7460\n",
      "Epoch 288/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9847 - acc: 0.7668 - val_loss: 0.9927 - val_acc: 0.7400\n",
      "Epoch 289/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9841 - acc: 0.7645 - val_loss: 0.9864 - val_acc: 0.7560\n",
      "Epoch 290/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9829 - acc: 0.7653 - val_loss: 0.9829 - val_acc: 0.7460\n",
      "Epoch 291/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9823 - acc: 0.7655 - val_loss: 0.9877 - val_acc: 0.7510\n",
      "Epoch 292/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9815 - acc: 0.7645 - val_loss: 0.9833 - val_acc: 0.7570\n",
      "Epoch 293/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9808 - acc: 0.7655 - val_loss: 0.9834 - val_acc: 0.7520\n",
      "Epoch 294/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9789 - acc: 0.7656 - val_loss: 0.9922 - val_acc: 0.7470\n",
      "Epoch 295/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9784 - acc: 0.7665 - val_loss: 0.9815 - val_acc: 0.7480\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9785 - acc: 0.7648 - val_loss: 0.9781 - val_acc: 0.7530\n",
      "Epoch 297/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9774 - acc: 0.7657 - val_loss: 0.9808 - val_acc: 0.7510\n",
      "Epoch 298/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9768 - acc: 0.7657 - val_loss: 0.9815 - val_acc: 0.7480\n",
      "Epoch 299/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9760 - acc: 0.7667 - val_loss: 0.9817 - val_acc: 0.7510\n",
      "Epoch 300/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9747 - acc: 0.7656 - val_loss: 0.9809 - val_acc: 0.7530\n",
      "Epoch 301/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9749 - acc: 0.7655 - val_loss: 0.9872 - val_acc: 0.7500\n",
      "Epoch 302/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9742 - acc: 0.7675 - val_loss: 0.9758 - val_acc: 0.7460\n",
      "Epoch 303/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9733 - acc: 0.7692 - val_loss: 0.9749 - val_acc: 0.7570\n",
      "Epoch 304/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9723 - acc: 0.7675 - val_loss: 0.9831 - val_acc: 0.7460\n",
      "Epoch 305/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9721 - acc: 0.7671 - val_loss: 0.9876 - val_acc: 0.7440\n",
      "Epoch 306/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9712 - acc: 0.7688 - val_loss: 0.9750 - val_acc: 0.7560\n",
      "Epoch 307/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9705 - acc: 0.7691 - val_loss: 0.9740 - val_acc: 0.7520\n",
      "Epoch 308/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9695 - acc: 0.7700 - val_loss: 0.9739 - val_acc: 0.7550\n",
      "Epoch 309/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9689 - acc: 0.7691 - val_loss: 0.9769 - val_acc: 0.7460\n",
      "Epoch 310/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9683 - acc: 0.7680 - val_loss: 0.9741 - val_acc: 0.7440\n",
      "Epoch 311/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9672 - acc: 0.7667 - val_loss: 0.9804 - val_acc: 0.7460\n",
      "Epoch 312/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9665 - acc: 0.7671 - val_loss: 0.9692 - val_acc: 0.7480\n",
      "Epoch 313/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9658 - acc: 0.7677 - val_loss: 0.9884 - val_acc: 0.7550\n",
      "Epoch 314/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9668 - acc: 0.7697 - val_loss: 0.9716 - val_acc: 0.7540\n",
      "Epoch 315/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9652 - acc: 0.7655 - val_loss: 0.9746 - val_acc: 0.7520\n",
      "Epoch 316/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9644 - acc: 0.7688 - val_loss: 0.9681 - val_acc: 0.7560\n",
      "Epoch 317/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9641 - acc: 0.7685 - val_loss: 0.9718 - val_acc: 0.7510\n",
      "Epoch 318/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9636 - acc: 0.7688 - val_loss: 0.9729 - val_acc: 0.7540\n",
      "Epoch 319/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9627 - acc: 0.7673 - val_loss: 0.9664 - val_acc: 0.7490\n",
      "Epoch 320/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9621 - acc: 0.7689 - val_loss: 0.9716 - val_acc: 0.7520\n",
      "Epoch 321/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9616 - acc: 0.7699 - val_loss: 0.9635 - val_acc: 0.7540\n",
      "Epoch 322/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9616 - acc: 0.7688 - val_loss: 0.9703 - val_acc: 0.7440\n",
      "Epoch 323/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9603 - acc: 0.7695 - val_loss: 0.9697 - val_acc: 0.7480\n",
      "Epoch 324/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9605 - acc: 0.7669 - val_loss: 0.9734 - val_acc: 0.7450\n",
      "Epoch 325/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9597 - acc: 0.7693 - val_loss: 0.9655 - val_acc: 0.7500\n",
      "Epoch 326/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9586 - acc: 0.7687 - val_loss: 0.9644 - val_acc: 0.7510\n",
      "Epoch 327/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9577 - acc: 0.7683 - val_loss: 0.9633 - val_acc: 0.7580\n",
      "Epoch 328/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9576 - acc: 0.7681 - val_loss: 0.9751 - val_acc: 0.7540\n",
      "Epoch 329/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9574 - acc: 0.7683 - val_loss: 0.9652 - val_acc: 0.7600\n",
      "Epoch 330/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9572 - acc: 0.7700 - val_loss: 0.9627 - val_acc: 0.7550\n",
      "Epoch 331/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9555 - acc: 0.7677 - val_loss: 0.9716 - val_acc: 0.7520\n",
      "Epoch 332/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9556 - acc: 0.7693 - val_loss: 0.9616 - val_acc: 0.7630\n",
      "Epoch 333/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9555 - acc: 0.7679 - val_loss: 0.9629 - val_acc: 0.7540\n",
      "Epoch 334/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9541 - acc: 0.7692 - val_loss: 0.9640 - val_acc: 0.7460\n",
      "Epoch 335/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9541 - acc: 0.7700 - val_loss: 0.9772 - val_acc: 0.7480\n",
      "Epoch 336/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9538 - acc: 0.7713 - val_loss: 0.9612 - val_acc: 0.7500\n",
      "Epoch 337/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9530 - acc: 0.7688 - val_loss: 0.9576 - val_acc: 0.7620\n",
      "Epoch 338/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9510 - acc: 0.7699 - val_loss: 0.9588 - val_acc: 0.7510\n",
      "Epoch 339/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9512 - acc: 0.7692 - val_loss: 0.9565 - val_acc: 0.7580\n",
      "Epoch 340/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9517 - acc: 0.7689 - val_loss: 0.9629 - val_acc: 0.7490\n",
      "Epoch 341/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9504 - acc: 0.7700 - val_loss: 0.9594 - val_acc: 0.7610\n",
      "Epoch 342/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9503 - acc: 0.7680 - val_loss: 0.9571 - val_acc: 0.7550\n",
      "Epoch 343/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9496 - acc: 0.7681 - val_loss: 0.9551 - val_acc: 0.7600\n",
      "Epoch 344/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9488 - acc: 0.7712 - val_loss: 0.9659 - val_acc: 0.7510\n",
      "Epoch 345/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9487 - acc: 0.7713 - val_loss: 0.9705 - val_acc: 0.7460\n",
      "Epoch 346/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9483 - acc: 0.7709 - val_loss: 0.9521 - val_acc: 0.7580\n",
      "Epoch 347/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9468 - acc: 0.7705 - val_loss: 0.9535 - val_acc: 0.7620\n",
      "Epoch 348/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9469 - acc: 0.7689 - val_loss: 0.9570 - val_acc: 0.7630\n",
      "Epoch 349/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9463 - acc: 0.7717 - val_loss: 0.9548 - val_acc: 0.7530\n",
      "Epoch 350/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9472 - acc: 0.7719 - val_loss: 0.9545 - val_acc: 0.7610\n",
      "Epoch 351/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9461 - acc: 0.7701 - val_loss: 0.9621 - val_acc: 0.7510\n",
      "Epoch 352/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9448 - acc: 0.7716 - val_loss: 0.9535 - val_acc: 0.7650\n",
      "Epoch 353/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9450 - acc: 0.7705 - val_loss: 0.9510 - val_acc: 0.7590\n",
      "Epoch 354/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9444 - acc: 0.7703 - val_loss: 0.9564 - val_acc: 0.7510\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9434 - acc: 0.7708 - val_loss: 0.9491 - val_acc: 0.7660\n",
      "Epoch 356/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9426 - acc: 0.7720 - val_loss: 0.9536 - val_acc: 0.7530\n",
      "Epoch 357/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9417 - acc: 0.7707 - val_loss: 0.9525 - val_acc: 0.7530\n",
      "Epoch 358/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9429 - acc: 0.7719 - val_loss: 0.9516 - val_acc: 0.7600\n",
      "Epoch 359/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9419 - acc: 0.7729 - val_loss: 0.9532 - val_acc: 0.7620\n",
      "Epoch 360/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.9450 - acc: 0.770 - 0s 44us/step - loss: 0.9427 - acc: 0.7716 - val_loss: 0.9465 - val_acc: 0.7610\n",
      "Epoch 361/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9407 - acc: 0.7715 - val_loss: 0.9475 - val_acc: 0.7570\n",
      "Epoch 362/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9408 - acc: 0.7723 - val_loss: 0.9450 - val_acc: 0.7630\n",
      "Epoch 363/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9394 - acc: 0.7712 - val_loss: 0.9461 - val_acc: 0.7660\n",
      "Epoch 364/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9392 - acc: 0.7729 - val_loss: 0.9468 - val_acc: 0.7550\n",
      "Epoch 365/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9388 - acc: 0.7712 - val_loss: 0.9449 - val_acc: 0.7540\n",
      "Epoch 366/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9387 - acc: 0.7731 - val_loss: 0.9479 - val_acc: 0.7640\n",
      "Epoch 367/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9381 - acc: 0.7711 - val_loss: 0.9654 - val_acc: 0.7400\n",
      "Epoch 368/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9390 - acc: 0.7708 - val_loss: 0.9449 - val_acc: 0.7600\n",
      "Epoch 369/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9373 - acc: 0.7705 - val_loss: 0.9459 - val_acc: 0.7630\n",
      "Epoch 370/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9374 - acc: 0.7713 - val_loss: 0.9447 - val_acc: 0.7660\n",
      "Epoch 371/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9356 - acc: 0.7717 - val_loss: 0.9426 - val_acc: 0.7650\n",
      "Epoch 372/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9351 - acc: 0.7747 - val_loss: 0.9450 - val_acc: 0.7640\n",
      "Epoch 373/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9352 - acc: 0.7708 - val_loss: 0.9525 - val_acc: 0.7530\n",
      "Epoch 374/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9345 - acc: 0.7724 - val_loss: 0.9514 - val_acc: 0.7500\n",
      "Epoch 375/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9351 - acc: 0.7739 - val_loss: 0.9410 - val_acc: 0.7650\n",
      "Epoch 376/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9336 - acc: 0.7701 - val_loss: 0.9415 - val_acc: 0.7610\n",
      "Epoch 377/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9341 - acc: 0.7712 - val_loss: 0.9454 - val_acc: 0.7580\n",
      "Epoch 378/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9335 - acc: 0.7724 - val_loss: 0.9444 - val_acc: 0.7510\n",
      "Epoch 379/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9326 - acc: 0.7729 - val_loss: 0.9418 - val_acc: 0.7610\n",
      "Epoch 380/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9327 - acc: 0.7732 - val_loss: 0.9391 - val_acc: 0.7640\n",
      "Epoch 381/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9313 - acc: 0.7736 - val_loss: 0.9389 - val_acc: 0.7680\n",
      "Epoch 382/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9310 - acc: 0.7736 - val_loss: 0.9373 - val_acc: 0.7640\n",
      "Epoch 383/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9310 - acc: 0.7708 - val_loss: 0.9374 - val_acc: 0.7620\n",
      "Epoch 384/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9302 - acc: 0.7748 - val_loss: 0.9408 - val_acc: 0.7600\n",
      "Epoch 385/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9297 - acc: 0.7739 - val_loss: 0.9381 - val_acc: 0.7620\n",
      "Epoch 386/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9301 - acc: 0.7727 - val_loss: 0.9401 - val_acc: 0.7540\n",
      "Epoch 387/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9296 - acc: 0.7729 - val_loss: 0.9441 - val_acc: 0.7550\n",
      "Epoch 388/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9287 - acc: 0.7724 - val_loss: 0.9380 - val_acc: 0.7680\n",
      "Epoch 389/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9281 - acc: 0.7731 - val_loss: 0.9398 - val_acc: 0.7630\n",
      "Epoch 390/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9281 - acc: 0.7731 - val_loss: 0.9389 - val_acc: 0.7570\n",
      "Epoch 391/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9277 - acc: 0.7737 - val_loss: 0.9360 - val_acc: 0.7630\n",
      "Epoch 392/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9273 - acc: 0.7733 - val_loss: 0.9399 - val_acc: 0.7600\n",
      "Epoch 393/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9268 - acc: 0.7725 - val_loss: 0.9360 - val_acc: 0.7590\n",
      "Epoch 394/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9270 - acc: 0.7736 - val_loss: 0.9385 - val_acc: 0.7550\n",
      "Epoch 395/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9268 - acc: 0.7729 - val_loss: 0.9420 - val_acc: 0.7660\n",
      "Epoch 396/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9260 - acc: 0.7745 - val_loss: 0.9486 - val_acc: 0.7580\n",
      "Epoch 397/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9260 - acc: 0.7741 - val_loss: 0.9360 - val_acc: 0.7610\n",
      "Epoch 398/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9244 - acc: 0.7737 - val_loss: 0.9329 - val_acc: 0.7590\n",
      "Epoch 399/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9237 - acc: 0.7745 - val_loss: 0.9386 - val_acc: 0.7650\n",
      "Epoch 400/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9235 - acc: 0.7752 - val_loss: 0.9405 - val_acc: 0.7580\n",
      "Epoch 401/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9235 - acc: 0.7735 - val_loss: 0.9440 - val_acc: 0.7600\n",
      "Epoch 402/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9241 - acc: 0.7725 - val_loss: 0.9371 - val_acc: 0.7580\n",
      "Epoch 403/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9232 - acc: 0.7736 - val_loss: 0.9312 - val_acc: 0.7650\n",
      "Epoch 404/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9228 - acc: 0.7744 - val_loss: 0.9327 - val_acc: 0.7630\n",
      "Epoch 405/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9222 - acc: 0.7721 - val_loss: 0.9306 - val_acc: 0.7700\n",
      "Epoch 406/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9211 - acc: 0.7735 - val_loss: 0.9347 - val_acc: 0.7560\n",
      "Epoch 407/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9210 - acc: 0.7735 - val_loss: 0.9288 - val_acc: 0.7670\n",
      "Epoch 408/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9204 - acc: 0.7720 - val_loss: 0.9274 - val_acc: 0.7680\n",
      "Epoch 409/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9204 - acc: 0.7763 - val_loss: 0.9447 - val_acc: 0.7620\n",
      "Epoch 410/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9207 - acc: 0.7756 - val_loss: 0.9429 - val_acc: 0.7510\n",
      "Epoch 411/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9194 - acc: 0.7733 - val_loss: 0.9497 - val_acc: 0.7530\n",
      "Epoch 412/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9207 - acc: 0.7737 - val_loss: 0.9272 - val_acc: 0.7650\n",
      "Epoch 413/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9191 - acc: 0.7760 - val_loss: 0.9390 - val_acc: 0.7590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 414/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9193 - acc: 0.7736 - val_loss: 0.9308 - val_acc: 0.7650\n",
      "Epoch 415/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9188 - acc: 0.7751 - val_loss: 0.9294 - val_acc: 0.7620\n",
      "Epoch 416/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9179 - acc: 0.7748 - val_loss: 0.9351 - val_acc: 0.7590\n",
      "Epoch 417/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9184 - acc: 0.7747 - val_loss: 0.9257 - val_acc: 0.7670\n",
      "Epoch 418/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9167 - acc: 0.7748 - val_loss: 0.9266 - val_acc: 0.7690\n",
      "Epoch 419/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9168 - acc: 0.7740 - val_loss: 0.9312 - val_acc: 0.7640\n",
      "Epoch 420/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9165 - acc: 0.7756 - val_loss: 0.9301 - val_acc: 0.7620\n",
      "Epoch 421/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9163 - acc: 0.7763 - val_loss: 0.9313 - val_acc: 0.7630\n",
      "Epoch 422/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9161 - acc: 0.7743 - val_loss: 0.9274 - val_acc: 0.7700\n",
      "Epoch 423/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9154 - acc: 0.7759 - val_loss: 0.9281 - val_acc: 0.7530\n",
      "Epoch 424/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9152 - acc: 0.7751 - val_loss: 0.9285 - val_acc: 0.7660\n",
      "Epoch 425/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9149 - acc: 0.7756 - val_loss: 0.9289 - val_acc: 0.7560\n",
      "Epoch 426/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9145 - acc: 0.7756 - val_loss: 0.9243 - val_acc: 0.7690\n",
      "Epoch 427/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9138 - acc: 0.7768 - val_loss: 0.9295 - val_acc: 0.7560\n",
      "Epoch 428/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9137 - acc: 0.7743 - val_loss: 0.9252 - val_acc: 0.7630\n",
      "Epoch 429/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9131 - acc: 0.7760 - val_loss: 0.9238 - val_acc: 0.7650\n",
      "Epoch 430/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9133 - acc: 0.7751 - val_loss: 0.9230 - val_acc: 0.7660\n",
      "Epoch 431/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9129 - acc: 0.7724 - val_loss: 0.9237 - val_acc: 0.7600\n",
      "Epoch 432/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9137 - acc: 0.7729 - val_loss: 0.9236 - val_acc: 0.7670\n",
      "Epoch 433/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9113 - acc: 0.7757 - val_loss: 0.9300 - val_acc: 0.7560\n",
      "Epoch 434/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9125 - acc: 0.7745 - val_loss: 0.9280 - val_acc: 0.7640\n",
      "Epoch 435/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9113 - acc: 0.7727 - val_loss: 0.9250 - val_acc: 0.7630\n",
      "Epoch 436/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9114 - acc: 0.7753 - val_loss: 0.9233 - val_acc: 0.7580\n",
      "Epoch 437/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9115 - acc: 0.7728 - val_loss: 0.9228 - val_acc: 0.7650\n",
      "Epoch 438/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9119 - acc: 0.7747 - val_loss: 0.9248 - val_acc: 0.7570\n",
      "Epoch 439/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9101 - acc: 0.7753 - val_loss: 0.9370 - val_acc: 0.7540\n",
      "Epoch 440/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9103 - acc: 0.7777 - val_loss: 0.9350 - val_acc: 0.7510\n",
      "Epoch 441/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9098 - acc: 0.7752 - val_loss: 0.9286 - val_acc: 0.7580\n",
      "Epoch 442/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9102 - acc: 0.7740 - val_loss: 0.9217 - val_acc: 0.7650\n",
      "Epoch 443/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9085 - acc: 0.7751 - val_loss: 0.9227 - val_acc: 0.7610\n",
      "Epoch 444/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9097 - acc: 0.7771 - val_loss: 0.9202 - val_acc: 0.7650\n",
      "Epoch 445/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9076 - acc: 0.7768 - val_loss: 0.9242 - val_acc: 0.7530\n",
      "Epoch 446/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9082 - acc: 0.7761 - val_loss: 0.9350 - val_acc: 0.7640\n",
      "Epoch 447/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9085 - acc: 0.7753 - val_loss: 0.9177 - val_acc: 0.7690\n",
      "Epoch 448/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9064 - acc: 0.7777 - val_loss: 0.9183 - val_acc: 0.7660\n",
      "Epoch 449/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9064 - acc: 0.7748 - val_loss: 0.9190 - val_acc: 0.7610\n",
      "Epoch 450/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9062 - acc: 0.7765 - val_loss: 0.9218 - val_acc: 0.7620\n",
      "Epoch 451/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9070 - acc: 0.7749 - val_loss: 0.9217 - val_acc: 0.7680\n",
      "Epoch 452/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9063 - acc: 0.7764 - val_loss: 0.9162 - val_acc: 0.7660\n",
      "Epoch 453/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9058 - acc: 0.7761 - val_loss: 0.9189 - val_acc: 0.7640\n",
      "Epoch 454/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9059 - acc: 0.7767 - val_loss: 0.9195 - val_acc: 0.7640\n",
      "Epoch 455/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9049 - acc: 0.7780 - val_loss: 0.9222 - val_acc: 0.7580\n",
      "Epoch 456/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9043 - acc: 0.7783 - val_loss: 0.9232 - val_acc: 0.7610\n",
      "Epoch 457/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9049 - acc: 0.7772 - val_loss: 0.9200 - val_acc: 0.7640\n",
      "Epoch 458/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9046 - acc: 0.7784 - val_loss: 0.9249 - val_acc: 0.7580\n",
      "Epoch 459/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9037 - acc: 0.7787 - val_loss: 0.9218 - val_acc: 0.7600\n",
      "Epoch 460/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9035 - acc: 0.7779 - val_loss: 0.9174 - val_acc: 0.7690\n",
      "Epoch 461/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9032 - acc: 0.7773 - val_loss: 0.9202 - val_acc: 0.7660\n",
      "Epoch 462/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9029 - acc: 0.7767 - val_loss: 0.9192 - val_acc: 0.7660\n",
      "Epoch 463/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9029 - acc: 0.7755 - val_loss: 0.9171 - val_acc: 0.7670\n",
      "Epoch 464/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9024 - acc: 0.7784 - val_loss: 0.9144 - val_acc: 0.7670\n",
      "Epoch 465/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9019 - acc: 0.7775 - val_loss: 0.9148 - val_acc: 0.7670\n",
      "Epoch 466/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9014 - acc: 0.7773 - val_loss: 0.9198 - val_acc: 0.7520\n",
      "Epoch 467/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9014 - acc: 0.7776 - val_loss: 0.9144 - val_acc: 0.7610\n",
      "Epoch 468/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9016 - acc: 0.7767 - val_loss: 0.9141 - val_acc: 0.7670\n",
      "Epoch 469/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9006 - acc: 0.7781 - val_loss: 0.9152 - val_acc: 0.7620\n",
      "Epoch 470/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9012 - acc: 0.7745 - val_loss: 0.9206 - val_acc: 0.7540\n",
      "Epoch 471/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8994 - acc: 0.7773 - val_loss: 0.9142 - val_acc: 0.7640\n",
      "Epoch 472/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9009 - acc: 0.7777 - val_loss: 0.9187 - val_acc: 0.7640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 473/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9009 - acc: 0.7785 - val_loss: 0.9140 - val_acc: 0.7620\n",
      "Epoch 474/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8999 - acc: 0.7775 - val_loss: 0.9120 - val_acc: 0.7670\n",
      "Epoch 475/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8987 - acc: 0.7792 - val_loss: 0.9345 - val_acc: 0.7490\n",
      "Epoch 476/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9005 - acc: 0.7776 - val_loss: 0.9133 - val_acc: 0.7700\n",
      "Epoch 477/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8992 - acc: 0.7785 - val_loss: 0.9191 - val_acc: 0.7580\n",
      "Epoch 478/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8987 - acc: 0.7788 - val_loss: 0.9105 - val_acc: 0.7640\n",
      "Epoch 479/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8982 - acc: 0.7780 - val_loss: 0.9209 - val_acc: 0.7580\n",
      "Epoch 480/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8986 - acc: 0.7792 - val_loss: 0.9102 - val_acc: 0.7650\n",
      "Epoch 481/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8975 - acc: 0.7781 - val_loss: 0.9128 - val_acc: 0.7690\n",
      "Epoch 482/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8969 - acc: 0.7769 - val_loss: 0.9133 - val_acc: 0.7560\n",
      "Epoch 483/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8982 - acc: 0.7776 - val_loss: 0.9196 - val_acc: 0.7680\n",
      "Epoch 484/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8989 - acc: 0.7796 - val_loss: 0.9117 - val_acc: 0.7690\n",
      "Epoch 485/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8973 - acc: 0.7791 - val_loss: 0.9126 - val_acc: 0.7600\n",
      "Epoch 486/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8967 - acc: 0.7781 - val_loss: 0.9115 - val_acc: 0.7590\n",
      "Epoch 487/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8961 - acc: 0.7791 - val_loss: 0.9100 - val_acc: 0.7660\n",
      "Epoch 488/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8964 - acc: 0.7771 - val_loss: 0.9090 - val_acc: 0.7660\n",
      "Epoch 489/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8956 - acc: 0.7788 - val_loss: 0.9136 - val_acc: 0.7560\n",
      "Epoch 490/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8959 - acc: 0.7793 - val_loss: 0.9222 - val_acc: 0.7490\n",
      "Epoch 491/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8955 - acc: 0.7792 - val_loss: 0.9107 - val_acc: 0.7670\n",
      "Epoch 492/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8950 - acc: 0.7809 - val_loss: 0.9091 - val_acc: 0.7700\n",
      "Epoch 493/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8940 - acc: 0.7764 - val_loss: 0.9135 - val_acc: 0.7680\n",
      "Epoch 494/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8941 - acc: 0.7785 - val_loss: 0.9099 - val_acc: 0.7600\n",
      "Epoch 495/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8928 - acc: 0.7781 - val_loss: 0.9082 - val_acc: 0.7660\n",
      "Epoch 496/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8932 - acc: 0.7785 - val_loss: 0.9113 - val_acc: 0.7620\n",
      "Epoch 497/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8926 - acc: 0.7807 - val_loss: 0.9074 - val_acc: 0.7710\n",
      "Epoch 498/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8939 - acc: 0.7772 - val_loss: 0.9051 - val_acc: 0.7680\n",
      "Epoch 499/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8925 - acc: 0.7781 - val_loss: 0.9156 - val_acc: 0.7650\n",
      "Epoch 500/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8932 - acc: 0.7793 - val_loss: 0.9136 - val_acc: 0.7660\n",
      "Epoch 501/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8932 - acc: 0.7797 - val_loss: 0.9077 - val_acc: 0.7670\n",
      "Epoch 502/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8929 - acc: 0.7780 - val_loss: 0.9101 - val_acc: 0.7660\n",
      "Epoch 503/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8921 - acc: 0.7788 - val_loss: 0.9118 - val_acc: 0.7580\n",
      "Epoch 504/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8917 - acc: 0.7781 - val_loss: 0.9090 - val_acc: 0.7700\n",
      "Epoch 505/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8913 - acc: 0.7797 - val_loss: 0.9057 - val_acc: 0.7640\n",
      "Epoch 506/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8922 - acc: 0.7783 - val_loss: 0.9152 - val_acc: 0.7520\n",
      "Epoch 507/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8911 - acc: 0.7776 - val_loss: 0.9081 - val_acc: 0.7620\n",
      "Epoch 508/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8917 - acc: 0.7783 - val_loss: 0.9075 - val_acc: 0.7630\n",
      "Epoch 509/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8913 - acc: 0.7773 - val_loss: 0.9061 - val_acc: 0.7660\n",
      "Epoch 510/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8903 - acc: 0.7791 - val_loss: 0.9047 - val_acc: 0.7680\n",
      "Epoch 511/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8894 - acc: 0.7792 - val_loss: 0.9070 - val_acc: 0.7620\n",
      "Epoch 512/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8905 - acc: 0.7783 - val_loss: 0.9043 - val_acc: 0.7690\n",
      "Epoch 513/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8899 - acc: 0.7793 - val_loss: 0.9092 - val_acc: 0.7670\n",
      "Epoch 514/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8912 - acc: 0.7788 - val_loss: 0.9194 - val_acc: 0.7610\n",
      "Epoch 515/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8890 - acc: 0.7799 - val_loss: 0.9028 - val_acc: 0.7710\n",
      "Epoch 516/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8892 - acc: 0.7804 - val_loss: 0.9060 - val_acc: 0.7610\n",
      "Epoch 517/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8895 - acc: 0.7793 - val_loss: 0.9112 - val_acc: 0.7640\n",
      "Epoch 518/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8895 - acc: 0.7792 - val_loss: 0.9020 - val_acc: 0.7700\n",
      "Epoch 519/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8881 - acc: 0.7779 - val_loss: 0.9022 - val_acc: 0.7650\n",
      "Epoch 520/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8873 - acc: 0.7775 - val_loss: 0.9023 - val_acc: 0.7700\n",
      "Epoch 521/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8875 - acc: 0.7791 - val_loss: 0.9098 - val_acc: 0.7650\n",
      "Epoch 522/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8873 - acc: 0.7819 - val_loss: 0.8998 - val_acc: 0.7700\n",
      "Epoch 523/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8862 - acc: 0.7803 - val_loss: 0.9126 - val_acc: 0.7620\n",
      "Epoch 524/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8866 - acc: 0.7807 - val_loss: 0.9049 - val_acc: 0.7710\n",
      "Epoch 525/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8878 - acc: 0.7803 - val_loss: 0.9050 - val_acc: 0.7620\n",
      "Epoch 526/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8868 - acc: 0.7817 - val_loss: 0.9017 - val_acc: 0.7600\n",
      "Epoch 527/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8861 - acc: 0.7813 - val_loss: 0.9029 - val_acc: 0.7710\n",
      "Epoch 528/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8866 - acc: 0.7809 - val_loss: 0.9049 - val_acc: 0.7630\n",
      "Epoch 529/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8865 - acc: 0.7808 - val_loss: 0.9038 - val_acc: 0.7690\n",
      "Epoch 530/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8863 - acc: 0.7809 - val_loss: 0.9004 - val_acc: 0.7650\n",
      "Epoch 531/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8860 - acc: 0.7809 - val_loss: 0.9061 - val_acc: 0.7650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 532/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8848 - acc: 0.7819 - val_loss: 0.9010 - val_acc: 0.7680\n",
      "Epoch 533/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8845 - acc: 0.7815 - val_loss: 0.9010 - val_acc: 0.7630\n",
      "Epoch 534/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8846 - acc: 0.7801 - val_loss: 0.9220 - val_acc: 0.7600\n",
      "Epoch 535/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8864 - acc: 0.7803 - val_loss: 0.9060 - val_acc: 0.7660\n",
      "Epoch 536/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8851 - acc: 0.7827 - val_loss: 0.9006 - val_acc: 0.7710\n",
      "Epoch 537/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8846 - acc: 0.7805 - val_loss: 0.9013 - val_acc: 0.7700\n",
      "Epoch 538/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8848 - acc: 0.7805 - val_loss: 0.9001 - val_acc: 0.7680\n",
      "Epoch 539/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8849 - acc: 0.7813 - val_loss: 0.8981 - val_acc: 0.7710\n",
      "Epoch 540/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8842 - acc: 0.7795 - val_loss: 0.9019 - val_acc: 0.7670\n",
      "Epoch 541/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8833 - acc: 0.7808 - val_loss: 0.8983 - val_acc: 0.7640\n",
      "Epoch 542/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8842 - acc: 0.7808 - val_loss: 0.8989 - val_acc: 0.7600\n",
      "Epoch 543/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8830 - acc: 0.7813 - val_loss: 0.9020 - val_acc: 0.7700\n",
      "Epoch 544/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8840 - acc: 0.7820 - val_loss: 0.9038 - val_acc: 0.7600\n",
      "Epoch 545/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8820 - acc: 0.7812 - val_loss: 0.9092 - val_acc: 0.7610\n",
      "Epoch 546/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8823 - acc: 0.7809 - val_loss: 0.8995 - val_acc: 0.7680\n",
      "Epoch 547/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8825 - acc: 0.7797 - val_loss: 0.9331 - val_acc: 0.7580\n",
      "Epoch 548/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8835 - acc: 0.7816 - val_loss: 0.8995 - val_acc: 0.7680\n",
      "Epoch 549/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8814 - acc: 0.7823 - val_loss: 0.9037 - val_acc: 0.7570\n",
      "Epoch 550/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8822 - acc: 0.7815 - val_loss: 0.8956 - val_acc: 0.7760\n",
      "Epoch 551/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8825 - acc: 0.7803 - val_loss: 0.9003 - val_acc: 0.7600\n",
      "Epoch 552/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8818 - acc: 0.7804 - val_loss: 0.8978 - val_acc: 0.7700\n",
      "Epoch 553/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8806 - acc: 0.7816 - val_loss: 0.8948 - val_acc: 0.7750\n",
      "Epoch 554/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8805 - acc: 0.7829 - val_loss: 0.9006 - val_acc: 0.7690\n",
      "Epoch 555/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8809 - acc: 0.7815 - val_loss: 0.8999 - val_acc: 0.7710\n",
      "Epoch 556/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8807 - acc: 0.7827 - val_loss: 0.8971 - val_acc: 0.7670\n",
      "Epoch 557/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8810 - acc: 0.7813 - val_loss: 0.8964 - val_acc: 0.7720\n",
      "Epoch 558/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8806 - acc: 0.7815 - val_loss: 0.9139 - val_acc: 0.7570\n",
      "Epoch 559/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8803 - acc: 0.7831 - val_loss: 0.8958 - val_acc: 0.7720\n",
      "Epoch 560/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8803 - acc: 0.7825 - val_loss: 0.9025 - val_acc: 0.7760\n",
      "Epoch 561/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8799 - acc: 0.7827 - val_loss: 0.8957 - val_acc: 0.7680\n",
      "Epoch 562/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8796 - acc: 0.7824 - val_loss: 0.8983 - val_acc: 0.7640\n",
      "Epoch 563/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8794 - acc: 0.7823 - val_loss: 0.8961 - val_acc: 0.7670\n",
      "Epoch 564/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.8789 - acc: 0.781 - 0s 36us/step - loss: 0.8798 - acc: 0.7816 - val_loss: 0.8959 - val_acc: 0.7700\n",
      "Epoch 565/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8789 - acc: 0.7817 - val_loss: 0.9079 - val_acc: 0.7670\n",
      "Epoch 566/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8780 - acc: 0.7828 - val_loss: 0.8931 - val_acc: 0.7660\n",
      "Epoch 567/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8794 - acc: 0.7817 - val_loss: 0.9093 - val_acc: 0.7570\n",
      "Epoch 568/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8796 - acc: 0.7829 - val_loss: 0.9164 - val_acc: 0.7620\n",
      "Epoch 569/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8798 - acc: 0.7803 - val_loss: 0.8987 - val_acc: 0.7660\n",
      "Epoch 570/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8778 - acc: 0.7827 - val_loss: 0.9133 - val_acc: 0.7520\n",
      "Epoch 571/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8782 - acc: 0.7827 - val_loss: 0.8960 - val_acc: 0.7670\n",
      "Epoch 572/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8773 - acc: 0.7812 - val_loss: 0.9114 - val_acc: 0.7610\n",
      "Epoch 573/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8788 - acc: 0.7813 - val_loss: 0.8938 - val_acc: 0.7700\n",
      "Epoch 574/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8767 - acc: 0.7835 - val_loss: 0.9009 - val_acc: 0.7680\n",
      "Epoch 575/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8770 - acc: 0.7852 - val_loss: 0.8958 - val_acc: 0.7730\n",
      "Epoch 576/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8760 - acc: 0.7825 - val_loss: 0.8958 - val_acc: 0.7720\n",
      "Epoch 577/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8765 - acc: 0.7804 - val_loss: 0.8937 - val_acc: 0.7740\n",
      "Epoch 578/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8771 - acc: 0.7832 - val_loss: 0.8957 - val_acc: 0.7680\n",
      "Epoch 579/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8775 - acc: 0.7815 - val_loss: 0.8947 - val_acc: 0.7720\n",
      "Epoch 580/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8756 - acc: 0.7839 - val_loss: 0.8932 - val_acc: 0.7710\n",
      "Epoch 581/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8765 - acc: 0.7823 - val_loss: 0.8949 - val_acc: 0.7690\n",
      "Epoch 582/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8767 - acc: 0.7823 - val_loss: 0.8966 - val_acc: 0.7650\n",
      "Epoch 583/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8755 - acc: 0.7831 - val_loss: 0.8939 - val_acc: 0.7660\n",
      "Epoch 584/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8768 - acc: 0.7817 - val_loss: 0.8959 - val_acc: 0.7700\n",
      "Epoch 585/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8773 - acc: 0.7820 - val_loss: 0.8954 - val_acc: 0.7690\n",
      "Epoch 586/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8759 - acc: 0.7800 - val_loss: 0.9110 - val_acc: 0.7630\n",
      "Epoch 587/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8749 - acc: 0.7825 - val_loss: 0.8942 - val_acc: 0.7660\n",
      "Epoch 588/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8749 - acc: 0.7833 - val_loss: 0.8952 - val_acc: 0.7670\n",
      "Epoch 589/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8751 - acc: 0.7833 - val_loss: 0.8930 - val_acc: 0.7680\n",
      "Epoch 590/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8733 - acc: 0.7859 - val_loss: 0.8997 - val_acc: 0.7570\n",
      "Epoch 591/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8744 - acc: 0.7843 - val_loss: 0.8956 - val_acc: 0.7630\n",
      "Epoch 592/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8740 - acc: 0.7827 - val_loss: 0.8945 - val_acc: 0.7690\n",
      "Epoch 593/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8744 - acc: 0.7829 - val_loss: 0.8928 - val_acc: 0.7660\n",
      "Epoch 594/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8747 - acc: 0.7817 - val_loss: 0.8969 - val_acc: 0.7700\n",
      "Epoch 595/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8738 - acc: 0.7829 - val_loss: 0.8980 - val_acc: 0.7670\n",
      "Epoch 596/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8726 - acc: 0.7845 - val_loss: 0.9185 - val_acc: 0.7620\n",
      "Epoch 597/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8742 - acc: 0.7841 - val_loss: 0.8928 - val_acc: 0.7720\n",
      "Epoch 598/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8733 - acc: 0.7853 - val_loss: 0.8896 - val_acc: 0.7700\n",
      "Epoch 599/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8725 - acc: 0.7813 - val_loss: 0.8941 - val_acc: 0.7650\n",
      "Epoch 600/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8729 - acc: 0.7840 - val_loss: 0.9022 - val_acc: 0.7620\n",
      "Epoch 601/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8732 - acc: 0.7815 - val_loss: 0.8955 - val_acc: 0.7600\n",
      "Epoch 602/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8727 - acc: 0.7836 - val_loss: 0.8885 - val_acc: 0.7710\n",
      "Epoch 603/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8718 - acc: 0.7837 - val_loss: 0.8904 - val_acc: 0.7700\n",
      "Epoch 604/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8715 - acc: 0.7837 - val_loss: 0.8936 - val_acc: 0.7660\n",
      "Epoch 605/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8722 - acc: 0.7815 - val_loss: 0.8932 - val_acc: 0.7660\n",
      "Epoch 606/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8735 - acc: 0.7813 - val_loss: 0.8924 - val_acc: 0.7670\n",
      "Epoch 607/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8708 - acc: 0.7855 - val_loss: 0.8929 - val_acc: 0.7710\n",
      "Epoch 608/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8717 - acc: 0.7835 - val_loss: 0.8895 - val_acc: 0.7740\n",
      "Epoch 609/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8707 - acc: 0.7851 - val_loss: 0.9019 - val_acc: 0.7520\n",
      "Epoch 610/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8706 - acc: 0.7836 - val_loss: 0.8904 - val_acc: 0.7680\n",
      "Epoch 611/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8717 - acc: 0.7833 - val_loss: 0.8919 - val_acc: 0.7690\n",
      "Epoch 612/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8708 - acc: 0.7847 - val_loss: 0.8945 - val_acc: 0.7730\n",
      "Epoch 613/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8701 - acc: 0.7823 - val_loss: 0.8916 - val_acc: 0.7700\n",
      "Epoch 614/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8695 - acc: 0.7828 - val_loss: 0.8978 - val_acc: 0.7550\n",
      "Epoch 615/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8715 - acc: 0.7839 - val_loss: 0.9058 - val_acc: 0.7530\n",
      "Epoch 616/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8698 - acc: 0.7824 - val_loss: 0.8964 - val_acc: 0.7680\n",
      "Epoch 617/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8692 - acc: 0.7851 - val_loss: 0.8976 - val_acc: 0.7690\n",
      "Epoch 618/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8707 - acc: 0.7844 - val_loss: 0.8921 - val_acc: 0.7620\n",
      "Epoch 619/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8695 - acc: 0.7840 - val_loss: 0.8903 - val_acc: 0.7720\n",
      "Epoch 620/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8695 - acc: 0.7839 - val_loss: 0.8968 - val_acc: 0.7590\n",
      "Epoch 621/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8697 - acc: 0.7833 - val_loss: 0.8901 - val_acc: 0.7650\n",
      "Epoch 622/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8685 - acc: 0.7851 - val_loss: 0.8876 - val_acc: 0.7730\n",
      "Epoch 623/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8696 - acc: 0.7837 - val_loss: 0.9169 - val_acc: 0.7610\n",
      "Epoch 624/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8693 - acc: 0.7831 - val_loss: 0.8896 - val_acc: 0.7770\n",
      "Epoch 625/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8682 - acc: 0.7828 - val_loss: 0.8882 - val_acc: 0.7670\n",
      "Epoch 626/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8690 - acc: 0.7803 - val_loss: 0.8892 - val_acc: 0.7680\n",
      "Epoch 627/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8687 - acc: 0.7824 - val_loss: 0.8875 - val_acc: 0.7770\n",
      "Epoch 628/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8687 - acc: 0.7839 - val_loss: 0.8971 - val_acc: 0.7640\n",
      "Epoch 629/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8680 - acc: 0.7848 - val_loss: 0.8876 - val_acc: 0.7740\n",
      "Epoch 630/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8674 - acc: 0.7847 - val_loss: 0.9123 - val_acc: 0.7540\n",
      "Epoch 631/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8684 - acc: 0.7851 - val_loss: 0.8878 - val_acc: 0.7690\n",
      "Epoch 632/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8683 - acc: 0.7845 - val_loss: 0.8944 - val_acc: 0.7520\n",
      "Epoch 633/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8670 - acc: 0.7845 - val_loss: 0.8923 - val_acc: 0.7690\n",
      "Epoch 634/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8683 - acc: 0.7837 - val_loss: 0.8988 - val_acc: 0.7670\n",
      "Epoch 635/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8667 - acc: 0.7852 - val_loss: 0.8988 - val_acc: 0.7700\n",
      "Epoch 636/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8675 - acc: 0.7851 - val_loss: 0.8952 - val_acc: 0.7700\n",
      "Epoch 637/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8669 - acc: 0.7843 - val_loss: 0.8854 - val_acc: 0.7690\n",
      "Epoch 638/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8669 - acc: 0.7835 - val_loss: 0.8869 - val_acc: 0.7720\n",
      "Epoch 639/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8668 - acc: 0.7824 - val_loss: 0.9022 - val_acc: 0.7690\n",
      "Epoch 640/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8677 - acc: 0.7831 - val_loss: 0.8940 - val_acc: 0.7730\n",
      "Epoch 641/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8654 - acc: 0.7859 - val_loss: 0.8909 - val_acc: 0.7730\n",
      "Epoch 642/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8671 - acc: 0.7841 - val_loss: 0.8867 - val_acc: 0.7720\n",
      "Epoch 643/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8649 - acc: 0.7823 - val_loss: 0.8942 - val_acc: 0.7660\n",
      "Epoch 644/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8665 - acc: 0.7848 - val_loss: 0.8929 - val_acc: 0.7740\n",
      "Epoch 645/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8648 - acc: 0.7843 - val_loss: 0.8940 - val_acc: 0.7730\n",
      "Epoch 646/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8660 - acc: 0.7836 - val_loss: 0.8881 - val_acc: 0.7660\n",
      "Epoch 647/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8643 - acc: 0.7864 - val_loss: 0.8970 - val_acc: 0.7670\n",
      "Epoch 648/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8653 - acc: 0.7841 - val_loss: 0.8851 - val_acc: 0.7760\n",
      "Epoch 649/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8649 - acc: 0.7836 - val_loss: 0.8896 - val_acc: 0.7760\n",
      "Epoch 650/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8640 - acc: 0.7841 - val_loss: 0.9073 - val_acc: 0.7550\n",
      "Epoch 651/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8668 - acc: 0.7856 - val_loss: 0.8923 - val_acc: 0.7630\n",
      "Epoch 652/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8658 - acc: 0.7857 - val_loss: 0.8906 - val_acc: 0.7760\n",
      "Epoch 653/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8642 - acc: 0.7855 - val_loss: 0.8862 - val_acc: 0.7560\n",
      "Epoch 654/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8643 - acc: 0.7841 - val_loss: 0.8937 - val_acc: 0.7660\n",
      "Epoch 655/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8647 - acc: 0.7861 - val_loss: 0.9076 - val_acc: 0.7620\n",
      "Epoch 656/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8655 - acc: 0.7853 - val_loss: 0.8893 - val_acc: 0.7700\n",
      "Epoch 657/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8645 - acc: 0.7832 - val_loss: 0.8870 - val_acc: 0.7620\n",
      "Epoch 658/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8646 - acc: 0.7843 - val_loss: 0.8907 - val_acc: 0.7620\n",
      "Epoch 659/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8649 - acc: 0.7856 - val_loss: 0.8835 - val_acc: 0.7760\n",
      "Epoch 660/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8632 - acc: 0.7845 - val_loss: 0.8857 - val_acc: 0.7690\n",
      "Epoch 661/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8629 - acc: 0.7832 - val_loss: 0.8880 - val_acc: 0.7660\n",
      "Epoch 662/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8625 - acc: 0.7867 - val_loss: 0.8946 - val_acc: 0.7670\n",
      "Epoch 663/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8632 - acc: 0.7867 - val_loss: 0.9199 - val_acc: 0.7470\n",
      "Epoch 664/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8648 - acc: 0.7844 - val_loss: 0.8890 - val_acc: 0.7710\n",
      "Epoch 665/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8626 - acc: 0.7867 - val_loss: 0.8937 - val_acc: 0.7620\n",
      "Epoch 666/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8642 - acc: 0.7847 - val_loss: 0.8906 - val_acc: 0.7690\n",
      "Epoch 667/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8622 - acc: 0.7860 - val_loss: 0.8846 - val_acc: 0.7710\n",
      "Epoch 668/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8621 - acc: 0.7855 - val_loss: 0.8870 - val_acc: 0.7670\n",
      "Epoch 669/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8630 - acc: 0.7855 - val_loss: 0.8827 - val_acc: 0.7710\n",
      "Epoch 670/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8619 - acc: 0.7847 - val_loss: 0.8839 - val_acc: 0.7690\n",
      "Epoch 671/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8638 - acc: 0.7860 - val_loss: 0.8872 - val_acc: 0.7560\n",
      "Epoch 672/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8615 - acc: 0.7848 - val_loss: 0.8840 - val_acc: 0.7770\n",
      "Epoch 673/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8606 - acc: 0.7865 - val_loss: 0.8842 - val_acc: 0.7670\n",
      "Epoch 674/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8600 - acc: 0.7875 - val_loss: 0.8842 - val_acc: 0.7740\n",
      "Epoch 675/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8627 - acc: 0.7840 - val_loss: 0.8854 - val_acc: 0.7720\n",
      "Epoch 676/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8617 - acc: 0.7847 - val_loss: 0.8840 - val_acc: 0.7750\n",
      "Epoch 677/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8607 - acc: 0.7855 - val_loss: 0.8852 - val_acc: 0.7750\n",
      "Epoch 678/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8610 - acc: 0.7844 - val_loss: 0.8845 - val_acc: 0.7740\n",
      "Epoch 679/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8604 - acc: 0.7869 - val_loss: 0.8938 - val_acc: 0.7700\n",
      "Epoch 680/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8603 - acc: 0.7865 - val_loss: 0.8844 - val_acc: 0.7720\n",
      "Epoch 681/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8603 - acc: 0.7857 - val_loss: 0.8933 - val_acc: 0.7670\n",
      "Epoch 682/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8608 - acc: 0.7860 - val_loss: 0.8809 - val_acc: 0.7740\n",
      "Epoch 683/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8609 - acc: 0.7847 - val_loss: 0.8915 - val_acc: 0.7640\n",
      "Epoch 684/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8605 - acc: 0.7871 - val_loss: 0.8904 - val_acc: 0.7600\n",
      "Epoch 685/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8605 - acc: 0.7867 - val_loss: 0.8816 - val_acc: 0.7760\n",
      "Epoch 686/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8610 - acc: 0.7833 - val_loss: 0.8809 - val_acc: 0.7740\n",
      "Epoch 687/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8585 - acc: 0.7853 - val_loss: 0.8882 - val_acc: 0.7700\n",
      "Epoch 688/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8602 - acc: 0.7852 - val_loss: 0.8810 - val_acc: 0.7760\n",
      "Epoch 689/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8584 - acc: 0.7859 - val_loss: 0.8847 - val_acc: 0.7660\n",
      "Epoch 690/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8604 - acc: 0.7852 - val_loss: 0.8929 - val_acc: 0.7670\n",
      "Epoch 691/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8613 - acc: 0.7852 - val_loss: 0.8826 - val_acc: 0.7730\n",
      "Epoch 692/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8590 - acc: 0.7857 - val_loss: 0.8836 - val_acc: 0.7750\n",
      "Epoch 693/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8594 - acc: 0.7865 - val_loss: 0.8780 - val_acc: 0.7750\n",
      "Epoch 694/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8584 - acc: 0.7837 - val_loss: 0.8789 - val_acc: 0.7720\n",
      "Epoch 695/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8569 - acc: 0.7856 - val_loss: 0.9019 - val_acc: 0.7580\n",
      "Epoch 696/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8586 - acc: 0.7864 - val_loss: 0.8833 - val_acc: 0.7680\n",
      "Epoch 697/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8587 - acc: 0.7867 - val_loss: 0.8867 - val_acc: 0.7670\n",
      "Epoch 698/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8589 - acc: 0.7884 - val_loss: 0.8854 - val_acc: 0.7630\n",
      "Epoch 699/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8580 - acc: 0.7863 - val_loss: 0.8930 - val_acc: 0.7600\n",
      "Epoch 700/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8586 - acc: 0.7864 - val_loss: 0.8877 - val_acc: 0.7720\n",
      "Epoch 701/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8589 - acc: 0.7864 - val_loss: 0.8848 - val_acc: 0.7670\n",
      "Epoch 702/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8573 - acc: 0.7863 - val_loss: 0.8811 - val_acc: 0.7800\n",
      "Epoch 703/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8580 - acc: 0.7875 - val_loss: 0.8932 - val_acc: 0.7660\n",
      "Epoch 704/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8581 - acc: 0.7884 - val_loss: 0.8864 - val_acc: 0.7630\n",
      "Epoch 705/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8568 - acc: 0.7860 - val_loss: 0.8818 - val_acc: 0.7700\n",
      "Epoch 706/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8582 - acc: 0.7865 - val_loss: 0.8820 - val_acc: 0.7780\n",
      "Epoch 707/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8578 - acc: 0.7875 - val_loss: 0.8899 - val_acc: 0.7640\n",
      "Epoch 708/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8567 - acc: 0.7865 - val_loss: 0.8828 - val_acc: 0.7630\n",
      "Epoch 709/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8559 - acc: 0.7872 - val_loss: 0.8801 - val_acc: 0.7730\n",
      "Epoch 710/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8576 - acc: 0.7848 - val_loss: 0.9000 - val_acc: 0.7660\n",
      "Epoch 711/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8566 - acc: 0.7859 - val_loss: 0.8860 - val_acc: 0.7610\n",
      "Epoch 712/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8575 - acc: 0.7876 - val_loss: 0.8777 - val_acc: 0.7790\n",
      "Epoch 713/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8551 - acc: 0.7885 - val_loss: 0.8820 - val_acc: 0.7680\n",
      "Epoch 714/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8577 - acc: 0.7881 - val_loss: 0.8791 - val_acc: 0.7750\n",
      "Epoch 715/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8567 - acc: 0.7861 - val_loss: 0.8865 - val_acc: 0.7650\n",
      "Epoch 716/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8570 - acc: 0.7872 - val_loss: 0.8798 - val_acc: 0.7790\n",
      "Epoch 717/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8555 - acc: 0.7875 - val_loss: 0.8794 - val_acc: 0.7690\n",
      "Epoch 718/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8556 - acc: 0.7888 - val_loss: 0.8802 - val_acc: 0.7760\n",
      "Epoch 719/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8554 - acc: 0.7879 - val_loss: 0.8878 - val_acc: 0.7720\n",
      "Epoch 720/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8542 - acc: 0.7892 - val_loss: 0.8827 - val_acc: 0.7630\n",
      "Epoch 721/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8553 - acc: 0.7869 - val_loss: 0.8850 - val_acc: 0.7740\n",
      "Epoch 722/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8555 - acc: 0.7856 - val_loss: 0.8881 - val_acc: 0.7750\n",
      "Epoch 723/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8531 - acc: 0.7888 - val_loss: 0.8868 - val_acc: 0.7720\n",
      "Epoch 724/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8547 - acc: 0.7872 - val_loss: 0.8804 - val_acc: 0.7740\n",
      "Epoch 725/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8552 - acc: 0.7867 - val_loss: 0.8799 - val_acc: 0.7660\n",
      "Epoch 726/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8550 - acc: 0.7875 - val_loss: 0.8773 - val_acc: 0.7790\n",
      "Epoch 727/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8541 - acc: 0.7867 - val_loss: 0.8778 - val_acc: 0.7790\n",
      "Epoch 728/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8556 - acc: 0.7880 - val_loss: 0.8768 - val_acc: 0.7740\n",
      "Epoch 729/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8545 - acc: 0.7876 - val_loss: 0.8845 - val_acc: 0.7570\n",
      "Epoch 730/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8534 - acc: 0.7879 - val_loss: 0.8850 - val_acc: 0.7700\n",
      "Epoch 731/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8546 - acc: 0.7881 - val_loss: 0.8927 - val_acc: 0.7640\n",
      "Epoch 732/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8547 - acc: 0.7888 - val_loss: 0.8765 - val_acc: 0.7770\n",
      "Epoch 733/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8542 - acc: 0.7877 - val_loss: 0.8846 - val_acc: 0.7620\n",
      "Epoch 734/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8529 - acc: 0.7884 - val_loss: 0.8797 - val_acc: 0.7690\n",
      "Epoch 735/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8535 - acc: 0.7871 - val_loss: 0.8825 - val_acc: 0.7690\n",
      "Epoch 736/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8541 - acc: 0.7883 - val_loss: 0.9048 - val_acc: 0.7520\n",
      "Epoch 737/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8541 - acc: 0.7857 - val_loss: 0.8794 - val_acc: 0.7720\n",
      "Epoch 738/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8543 - acc: 0.7877 - val_loss: 0.8813 - val_acc: 0.7730\n",
      "Epoch 739/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8521 - acc: 0.7877 - val_loss: 0.8790 - val_acc: 0.7610\n",
      "Epoch 740/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8530 - acc: 0.7861 - val_loss: 0.8771 - val_acc: 0.7790\n",
      "Epoch 741/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8522 - acc: 0.7885 - val_loss: 0.8771 - val_acc: 0.7750\n",
      "Epoch 742/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8530 - acc: 0.7875 - val_loss: 0.8800 - val_acc: 0.7770\n",
      "Epoch 743/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8522 - acc: 0.7863 - val_loss: 0.8798 - val_acc: 0.7620\n",
      "Epoch 744/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8517 - acc: 0.7883 - val_loss: 0.8860 - val_acc: 0.7620\n",
      "Epoch 745/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8534 - acc: 0.7867 - val_loss: 0.8824 - val_acc: 0.7680\n",
      "Epoch 746/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8517 - acc: 0.7897 - val_loss: 0.8847 - val_acc: 0.7670\n",
      "Epoch 747/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8513 - acc: 0.7877 - val_loss: 0.8908 - val_acc: 0.7630\n",
      "Epoch 748/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8533 - acc: 0.7873 - val_loss: 0.8952 - val_acc: 0.7640\n",
      "Epoch 749/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8515 - acc: 0.7877 - val_loss: 0.9081 - val_acc: 0.7580\n",
      "Epoch 750/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8524 - acc: 0.7869 - val_loss: 0.8971 - val_acc: 0.7570\n",
      "Epoch 751/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8524 - acc: 0.7879 - val_loss: 0.8761 - val_acc: 0.7690\n",
      "Epoch 752/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8515 - acc: 0.7881 - val_loss: 0.8807 - val_acc: 0.7690\n",
      "Epoch 753/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8515 - acc: 0.7876 - val_loss: 0.8847 - val_acc: 0.7680\n",
      "Epoch 754/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8525 - acc: 0.7901 - val_loss: 0.8848 - val_acc: 0.7630\n",
      "Epoch 755/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8516 - acc: 0.7891 - val_loss: 0.8805 - val_acc: 0.7660\n",
      "Epoch 756/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8515 - acc: 0.7881 - val_loss: 0.8771 - val_acc: 0.7740\n",
      "Epoch 757/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8502 - acc: 0.7880 - val_loss: 0.8824 - val_acc: 0.7660\n",
      "Epoch 758/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8504 - acc: 0.7897 - val_loss: 0.8898 - val_acc: 0.7520\n",
      "Epoch 759/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8512 - acc: 0.7885 - val_loss: 0.8823 - val_acc: 0.7660\n",
      "Epoch 760/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8509 - acc: 0.7871 - val_loss: 0.8852 - val_acc: 0.7650\n",
      "Epoch 761/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8501 - acc: 0.7884 - val_loss: 0.8866 - val_acc: 0.7630\n",
      "Epoch 762/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8504 - acc: 0.7880 - val_loss: 0.9056 - val_acc: 0.7660\n",
      "Epoch 763/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8526 - acc: 0.7872 - val_loss: 0.8780 - val_acc: 0.7710\n",
      "Epoch 764/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8492 - acc: 0.7908 - val_loss: 0.9256 - val_acc: 0.7600\n",
      "Epoch 765/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8505 - acc: 0.7871 - val_loss: 0.8952 - val_acc: 0.7630\n",
      "Epoch 766/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8510 - acc: 0.7888 - val_loss: 0.8824 - val_acc: 0.7680\n",
      "Epoch 767/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8502 - acc: 0.7888 - val_loss: 0.8754 - val_acc: 0.7660\n",
      "Epoch 768/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8494 - acc: 0.7873 - val_loss: 0.8806 - val_acc: 0.7650\n",
      "Epoch 769/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8499 - acc: 0.7888 - val_loss: 0.8784 - val_acc: 0.7610\n",
      "Epoch 770/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8493 - acc: 0.7896 - val_loss: 0.8749 - val_acc: 0.7730\n",
      "Epoch 771/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8489 - acc: 0.7883 - val_loss: 0.8865 - val_acc: 0.7570\n",
      "Epoch 772/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8503 - acc: 0.7861 - val_loss: 0.8771 - val_acc: 0.7730\n",
      "Epoch 773/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8514 - acc: 0.7860 - val_loss: 0.8727 - val_acc: 0.7780\n",
      "Epoch 774/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8473 - acc: 0.7901 - val_loss: 0.8729 - val_acc: 0.7810\n",
      "Epoch 775/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8500 - acc: 0.7881 - val_loss: 0.8779 - val_acc: 0.7670\n",
      "Epoch 776/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8501 - acc: 0.7884 - val_loss: 0.8944 - val_acc: 0.7730\n",
      "Epoch 777/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8512 - acc: 0.7875 - val_loss: 0.8777 - val_acc: 0.7730\n",
      "Epoch 778/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8491 - acc: 0.7883 - val_loss: 0.8754 - val_acc: 0.7670\n",
      "Epoch 779/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8485 - acc: 0.7908 - val_loss: 0.8877 - val_acc: 0.7600\n",
      "Epoch 780/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8483 - acc: 0.7869 - val_loss: 0.8789 - val_acc: 0.7720\n",
      "Epoch 781/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8484 - acc: 0.7879 - val_loss: 0.8857 - val_acc: 0.7590\n",
      "Epoch 782/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8477 - acc: 0.7885 - val_loss: 0.8737 - val_acc: 0.7710\n",
      "Epoch 783/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8476 - acc: 0.7881 - val_loss: 0.8722 - val_acc: 0.7660\n",
      "Epoch 784/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8476 - acc: 0.7900 - val_loss: 0.8985 - val_acc: 0.7710\n",
      "Epoch 785/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8482 - acc: 0.7865 - val_loss: 0.8861 - val_acc: 0.7640\n",
      "Epoch 786/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8490 - acc: 0.7897 - val_loss: 0.8863 - val_acc: 0.7690\n",
      "Epoch 787/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8481 - acc: 0.7872 - val_loss: 0.8863 - val_acc: 0.7790\n",
      "Epoch 788/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8472 - acc: 0.7884 - val_loss: 0.8747 - val_acc: 0.7760\n",
      "Epoch 789/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8478 - acc: 0.7921 - val_loss: 0.8806 - val_acc: 0.7730\n",
      "Epoch 790/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8463 - acc: 0.7892 - val_loss: 0.9155 - val_acc: 0.7620\n",
      "Epoch 791/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8494 - acc: 0.7888 - val_loss: 0.8986 - val_acc: 0.7580\n",
      "Epoch 792/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8471 - acc: 0.7880 - val_loss: 0.8724 - val_acc: 0.7720\n",
      "Epoch 793/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8464 - acc: 0.7911 - val_loss: 0.8766 - val_acc: 0.7700\n",
      "Epoch 794/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8464 - acc: 0.7901 - val_loss: 0.8803 - val_acc: 0.7680\n",
      "Epoch 795/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8463 - acc: 0.7919 - val_loss: 0.8756 - val_acc: 0.7760\n",
      "Epoch 796/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8457 - acc: 0.7896 - val_loss: 0.8704 - val_acc: 0.7780\n",
      "Epoch 797/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8467 - acc: 0.7897 - val_loss: 0.8741 - val_acc: 0.7680\n",
      "Epoch 798/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8451 - acc: 0.7889 - val_loss: 0.8812 - val_acc: 0.7730\n",
      "Epoch 799/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8475 - acc: 0.7888 - val_loss: 0.8841 - val_acc: 0.7680\n",
      "Epoch 800/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8459 - acc: 0.7901 - val_loss: 0.8758 - val_acc: 0.7640\n",
      "Epoch 801/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8454 - acc: 0.7911 - val_loss: 0.8855 - val_acc: 0.7680\n",
      "Epoch 802/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8478 - acc: 0.7895 - val_loss: 0.8714 - val_acc: 0.7760\n",
      "Epoch 803/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8447 - acc: 0.7931 - val_loss: 0.8870 - val_acc: 0.7630\n",
      "Epoch 804/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8463 - acc: 0.7917 - val_loss: 0.8808 - val_acc: 0.7760\n",
      "Epoch 805/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8469 - acc: 0.7899 - val_loss: 0.8759 - val_acc: 0.7670\n",
      "Epoch 806/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8457 - acc: 0.7880 - val_loss: 0.8780 - val_acc: 0.7730\n",
      "Epoch 807/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8454 - acc: 0.7921 - val_loss: 0.8760 - val_acc: 0.7710\n",
      "Epoch 808/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8456 - acc: 0.7892 - val_loss: 0.8776 - val_acc: 0.7700\n",
      "Epoch 809/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8449 - acc: 0.7893 - val_loss: 0.8746 - val_acc: 0.7680\n",
      "Epoch 810/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8450 - acc: 0.7915 - val_loss: 0.8837 - val_acc: 0.7720\n",
      "Epoch 811/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8455 - acc: 0.7911 - val_loss: 0.8805 - val_acc: 0.7600\n",
      "Epoch 812/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8452 - acc: 0.7899 - val_loss: 0.8708 - val_acc: 0.7720\n",
      "Epoch 813/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8440 - acc: 0.7892 - val_loss: 0.8731 - val_acc: 0.7820\n",
      "Epoch 814/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8433 - acc: 0.7921 - val_loss: 0.8840 - val_acc: 0.7670\n",
      "Epoch 815/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8447 - acc: 0.7893 - val_loss: 0.8840 - val_acc: 0.7670\n",
      "Epoch 816/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8438 - acc: 0.7905 - val_loss: 0.8796 - val_acc: 0.7650\n",
      "Epoch 817/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8442 - acc: 0.7887 - val_loss: 0.8811 - val_acc: 0.7710\n",
      "Epoch 818/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8439 - acc: 0.7887 - val_loss: 0.8814 - val_acc: 0.7660\n",
      "Epoch 819/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8444 - acc: 0.7895 - val_loss: 0.9006 - val_acc: 0.7600\n",
      "Epoch 820/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8439 - acc: 0.7895 - val_loss: 0.8831 - val_acc: 0.7550\n",
      "Epoch 821/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8427 - acc: 0.7885 - val_loss: 0.8728 - val_acc: 0.7700\n",
      "Epoch 822/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8431 - acc: 0.7927 - val_loss: 0.8718 - val_acc: 0.7760\n",
      "Epoch 823/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8430 - acc: 0.7899 - val_loss: 0.8770 - val_acc: 0.7700\n",
      "Epoch 824/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8447 - acc: 0.7880 - val_loss: 0.8745 - val_acc: 0.7650\n",
      "Epoch 825/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8428 - acc: 0.7901 - val_loss: 0.8729 - val_acc: 0.7760\n",
      "Epoch 826/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8425 - acc: 0.7915 - val_loss: 0.8753 - val_acc: 0.7740\n",
      "Epoch 827/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8437 - acc: 0.7893 - val_loss: 0.8756 - val_acc: 0.7630\n",
      "Epoch 828/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8419 - acc: 0.7913 - val_loss: 0.8763 - val_acc: 0.7740\n",
      "Epoch 829/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8435 - acc: 0.7900 - val_loss: 0.8850 - val_acc: 0.7780\n",
      "Epoch 830/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8432 - acc: 0.7911 - val_loss: 0.8700 - val_acc: 0.7760\n",
      "Epoch 831/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8421 - acc: 0.7883 - val_loss: 0.8707 - val_acc: 0.7790\n",
      "Epoch 832/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8446 - acc: 0.7892 - val_loss: 0.8699 - val_acc: 0.7750\n",
      "Epoch 833/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8420 - acc: 0.7919 - val_loss: 0.8727 - val_acc: 0.7700\n",
      "Epoch 834/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8420 - acc: 0.7923 - val_loss: 0.8736 - val_acc: 0.7640\n",
      "Epoch 835/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8415 - acc: 0.7896 - val_loss: 0.8838 - val_acc: 0.7550\n",
      "Epoch 836/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8415 - acc: 0.7917 - val_loss: 0.8744 - val_acc: 0.7670\n",
      "Epoch 837/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8430 - acc: 0.7892 - val_loss: 0.8892 - val_acc: 0.7550\n",
      "Epoch 838/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8431 - acc: 0.7903 - val_loss: 0.8729 - val_acc: 0.7710\n",
      "Epoch 839/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8410 - acc: 0.7931 - val_loss: 0.8717 - val_acc: 0.7700\n",
      "Epoch 840/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8413 - acc: 0.7903 - val_loss: 0.8742 - val_acc: 0.7740\n",
      "Epoch 841/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8417 - acc: 0.7908 - val_loss: 0.8823 - val_acc: 0.7750\n",
      "Epoch 842/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8423 - acc: 0.7908 - val_loss: 0.8700 - val_acc: 0.7700\n",
      "Epoch 843/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8403 - acc: 0.7907 - val_loss: 0.8698 - val_acc: 0.7720\n",
      "Epoch 844/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8405 - acc: 0.7913 - val_loss: 0.8691 - val_acc: 0.7740\n",
      "Epoch 845/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8429 - acc: 0.7896 - val_loss: 0.8698 - val_acc: 0.7740\n",
      "Epoch 846/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8391 - acc: 0.7912 - val_loss: 0.8722 - val_acc: 0.7700\n",
      "Epoch 847/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8410 - acc: 0.7901 - val_loss: 0.9618 - val_acc: 0.7380\n",
      "Epoch 848/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8452 - acc: 0.7873 - val_loss: 0.8821 - val_acc: 0.7790\n",
      "Epoch 849/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8439 - acc: 0.7900 - val_loss: 0.8696 - val_acc: 0.7820\n",
      "Epoch 850/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8402 - acc: 0.7912 - val_loss: 0.8697 - val_acc: 0.7610\n",
      "Epoch 851/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8392 - acc: 0.7917 - val_loss: 0.8730 - val_acc: 0.7640\n",
      "Epoch 852/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8392 - acc: 0.7928 - val_loss: 0.8805 - val_acc: 0.7570\n",
      "Epoch 853/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8410 - acc: 0.7911 - val_loss: 0.8692 - val_acc: 0.7690\n",
      "Epoch 854/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8393 - acc: 0.7912 - val_loss: 0.8701 - val_acc: 0.7720\n",
      "Epoch 855/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8404 - acc: 0.7904 - val_loss: 0.8670 - val_acc: 0.7790\n",
      "Epoch 856/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8417 - acc: 0.7921 - val_loss: 0.8851 - val_acc: 0.7690\n",
      "Epoch 857/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8400 - acc: 0.7905 - val_loss: 0.8999 - val_acc: 0.7610\n",
      "Epoch 858/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8409 - acc: 0.7907 - val_loss: 0.8827 - val_acc: 0.7700\n",
      "Epoch 859/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8418 - acc: 0.7880 - val_loss: 0.8833 - val_acc: 0.7710\n",
      "Epoch 860/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8380 - acc: 0.7939 - val_loss: 0.8869 - val_acc: 0.7600\n",
      "Epoch 861/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8395 - acc: 0.7927 - val_loss: 0.8714 - val_acc: 0.7600\n",
      "Epoch 862/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8386 - acc: 0.7903 - val_loss: 0.8788 - val_acc: 0.7680\n",
      "Epoch 863/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8409 - acc: 0.7909 - val_loss: 0.8762 - val_acc: 0.7700\n",
      "Epoch 864/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8391 - acc: 0.7908 - val_loss: 0.8687 - val_acc: 0.7720\n",
      "Epoch 865/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8390 - acc: 0.7929 - val_loss: 0.8948 - val_acc: 0.7550\n",
      "Epoch 866/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8384 - acc: 0.7925 - val_loss: 0.8860 - val_acc: 0.7680\n",
      "Epoch 867/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8376 - acc: 0.7923 - val_loss: 0.8703 - val_acc: 0.7740\n",
      "Epoch 868/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8389 - acc: 0.7901 - val_loss: 0.8712 - val_acc: 0.7650\n",
      "Epoch 869/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8403 - acc: 0.7921 - val_loss: 0.8751 - val_acc: 0.7640\n",
      "Epoch 870/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8391 - acc: 0.7933 - val_loss: 0.8676 - val_acc: 0.7740\n",
      "Epoch 871/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8390 - acc: 0.7920 - val_loss: 0.8739 - val_acc: 0.7710\n",
      "Epoch 872/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8379 - acc: 0.7924 - val_loss: 0.8740 - val_acc: 0.7750\n",
      "Epoch 873/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8370 - acc: 0.7909 - val_loss: 0.8694 - val_acc: 0.7780\n",
      "Epoch 874/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8384 - acc: 0.7924 - val_loss: 0.8706 - val_acc: 0.7780\n",
      "Epoch 875/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8360 - acc: 0.7919 - val_loss: 0.8774 - val_acc: 0.7750\n",
      "Epoch 876/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8364 - acc: 0.7953 - val_loss: 0.8750 - val_acc: 0.7700\n",
      "Epoch 877/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8387 - acc: 0.7908 - val_loss: 0.8785 - val_acc: 0.7660\n",
      "Epoch 878/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8378 - acc: 0.7920 - val_loss: 0.8692 - val_acc: 0.7620\n",
      "Epoch 879/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8357 - acc: 0.7912 - val_loss: 0.8671 - val_acc: 0.7770\n",
      "Epoch 880/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8378 - acc: 0.7921 - val_loss: 0.8758 - val_acc: 0.7720\n",
      "Epoch 881/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8380 - acc: 0.7924 - val_loss: 0.8648 - val_acc: 0.7760\n",
      "Epoch 882/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8372 - acc: 0.7929 - val_loss: 0.8806 - val_acc: 0.7570\n",
      "Epoch 883/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8369 - acc: 0.7915 - val_loss: 0.8671 - val_acc: 0.7760\n",
      "Epoch 884/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8381 - acc: 0.7924 - val_loss: 0.8780 - val_acc: 0.7590\n",
      "Epoch 885/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8364 - acc: 0.7929 - val_loss: 0.8660 - val_acc: 0.7780\n",
      "Epoch 886/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8360 - acc: 0.7935 - val_loss: 0.8719 - val_acc: 0.7640\n",
      "Epoch 887/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8375 - acc: 0.7937 - val_loss: 0.8741 - val_acc: 0.7720\n",
      "Epoch 888/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8369 - acc: 0.7924 - val_loss: 0.8723 - val_acc: 0.7700\n",
      "Epoch 889/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8362 - acc: 0.7937 - val_loss: 0.8802 - val_acc: 0.7640\n",
      "Epoch 890/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8361 - acc: 0.7883 - val_loss: 0.8674 - val_acc: 0.7750\n",
      "Epoch 891/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8349 - acc: 0.7940 - val_loss: 0.8714 - val_acc: 0.7690\n",
      "Epoch 892/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8356 - acc: 0.7939 - val_loss: 0.8732 - val_acc: 0.7630\n",
      "Epoch 893/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8354 - acc: 0.7935 - val_loss: 0.8716 - val_acc: 0.7700\n",
      "Epoch 894/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8361 - acc: 0.7916 - val_loss: 0.8665 - val_acc: 0.7740\n",
      "Epoch 895/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8353 - acc: 0.7949 - val_loss: 0.8713 - val_acc: 0.7790\n",
      "Epoch 896/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8348 - acc: 0.7917 - val_loss: 0.8746 - val_acc: 0.7700\n",
      "Epoch 897/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8351 - acc: 0.7912 - val_loss: 0.8771 - val_acc: 0.7750\n",
      "Epoch 898/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8370 - acc: 0.7927 - val_loss: 0.8718 - val_acc: 0.7730\n",
      "Epoch 899/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8361 - acc: 0.7931 - val_loss: 0.8860 - val_acc: 0.7580\n",
      "Epoch 900/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8353 - acc: 0.7915 - val_loss: 0.8719 - val_acc: 0.7750\n",
      "Epoch 901/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8356 - acc: 0.7935 - val_loss: 0.8811 - val_acc: 0.7600\n",
      "Epoch 902/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8350 - acc: 0.7932 - val_loss: 0.8670 - val_acc: 0.7700\n",
      "Epoch 903/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8361 - acc: 0.7917 - val_loss: 0.8775 - val_acc: 0.7670\n",
      "Epoch 904/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8366 - acc: 0.7927 - val_loss: 0.8785 - val_acc: 0.7630\n",
      "Epoch 905/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8349 - acc: 0.7931 - val_loss: 0.8660 - val_acc: 0.7820\n",
      "Epoch 906/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8348 - acc: 0.7925 - val_loss: 0.8749 - val_acc: 0.7630\n",
      "Epoch 907/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8355 - acc: 0.7924 - val_loss: 0.8714 - val_acc: 0.7740\n",
      "Epoch 908/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8357 - acc: 0.7931 - val_loss: 0.8642 - val_acc: 0.7800\n",
      "Epoch 909/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8336 - acc: 0.7924 - val_loss: 0.8703 - val_acc: 0.7630\n",
      "Epoch 910/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8331 - acc: 0.7953 - val_loss: 0.8805 - val_acc: 0.7660\n",
      "Epoch 911/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8329 - acc: 0.7939 - val_loss: 0.8785 - val_acc: 0.7590\n",
      "Epoch 912/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8348 - acc: 0.7917 - val_loss: 0.8721 - val_acc: 0.7800\n",
      "Epoch 913/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8345 - acc: 0.7949 - val_loss: 0.9010 - val_acc: 0.7670\n",
      "Epoch 914/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8361 - acc: 0.7913 - val_loss: 0.8663 - val_acc: 0.7790\n",
      "Epoch 915/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8332 - acc: 0.7947 - val_loss: 0.8676 - val_acc: 0.7700\n",
      "Epoch 916/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8328 - acc: 0.7931 - val_loss: 0.8873 - val_acc: 0.7640\n",
      "Epoch 917/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8343 - acc: 0.7940 - val_loss: 0.8788 - val_acc: 0.7690\n",
      "Epoch 918/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8333 - acc: 0.7933 - val_loss: 0.9082 - val_acc: 0.7560\n",
      "Epoch 919/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8334 - acc: 0.7937 - val_loss: 0.8792 - val_acc: 0.7650\n",
      "Epoch 920/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8334 - acc: 0.7905 - val_loss: 0.8708 - val_acc: 0.7700\n",
      "Epoch 921/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8318 - acc: 0.7947 - val_loss: 0.8702 - val_acc: 0.7780\n",
      "Epoch 922/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8331 - acc: 0.7961 - val_loss: 0.8727 - val_acc: 0.7710\n",
      "Epoch 923/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8334 - acc: 0.7940 - val_loss: 0.8657 - val_acc: 0.7670\n",
      "Epoch 924/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8320 - acc: 0.7917 - val_loss: 0.8748 - val_acc: 0.7620\n",
      "Epoch 925/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8348 - acc: 0.7956 - val_loss: 0.8673 - val_acc: 0.7710\n",
      "Epoch 926/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8332 - acc: 0.7928 - val_loss: 0.8731 - val_acc: 0.7700\n",
      "Epoch 927/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8332 - acc: 0.7948 - val_loss: 0.8649 - val_acc: 0.7750\n",
      "Epoch 928/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8340 - acc: 0.7935 - val_loss: 0.8692 - val_acc: 0.7730\n",
      "Epoch 929/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8319 - acc: 0.7949 - val_loss: 0.8743 - val_acc: 0.7750\n",
      "Epoch 930/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8353 - acc: 0.7935 - val_loss: 0.8815 - val_acc: 0.7590\n",
      "Epoch 931/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8325 - acc: 0.7924 - val_loss: 0.8768 - val_acc: 0.7660\n",
      "Epoch 932/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8309 - acc: 0.7928 - val_loss: 0.8755 - val_acc: 0.7800\n",
      "Epoch 933/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8327 - acc: 0.7940 - val_loss: 0.8750 - val_acc: 0.7760\n",
      "Epoch 934/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8318 - acc: 0.7943 - val_loss: 0.8758 - val_acc: 0.7630\n",
      "Epoch 935/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8316 - acc: 0.7937 - val_loss: 0.8810 - val_acc: 0.7650\n",
      "Epoch 936/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8310 - acc: 0.7932 - val_loss: 0.8650 - val_acc: 0.7790\n",
      "Epoch 937/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8313 - acc: 0.7931 - val_loss: 0.8739 - val_acc: 0.7790\n",
      "Epoch 938/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8301 - acc: 0.7935 - val_loss: 0.8809 - val_acc: 0.7660\n",
      "Epoch 939/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8326 - acc: 0.7952 - val_loss: 0.8729 - val_acc: 0.7660\n",
      "Epoch 940/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8318 - acc: 0.7915 - val_loss: 0.8750 - val_acc: 0.7770\n",
      "Epoch 941/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8314 - acc: 0.7937 - val_loss: 0.8660 - val_acc: 0.7780\n",
      "Epoch 942/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8322 - acc: 0.7944 - val_loss: 0.8653 - val_acc: 0.7800\n",
      "Epoch 943/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8303 - acc: 0.7969 - val_loss: 0.8620 - val_acc: 0.7780\n",
      "Epoch 944/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8307 - acc: 0.7940 - val_loss: 0.9117 - val_acc: 0.7530\n",
      "Epoch 945/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8319 - acc: 0.7949 - val_loss: 0.8722 - val_acc: 0.7640\n",
      "Epoch 946/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8318 - acc: 0.7956 - val_loss: 0.9060 - val_acc: 0.7530\n",
      "Epoch 947/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8312 - acc: 0.7940 - val_loss: 0.8882 - val_acc: 0.7560\n",
      "Epoch 948/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8293 - acc: 0.7953 - val_loss: 0.8652 - val_acc: 0.7710\n",
      "Epoch 949/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8320 - acc: 0.7960 - val_loss: 0.8838 - val_acc: 0.7720\n",
      "Epoch 950/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8297 - acc: 0.7943 - val_loss: 0.8648 - val_acc: 0.7820\n",
      "Epoch 951/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8288 - acc: 0.7957 - val_loss: 0.8646 - val_acc: 0.7750\n",
      "Epoch 952/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8293 - acc: 0.7955 - val_loss: 0.8815 - val_acc: 0.7700\n",
      "Epoch 953/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8303 - acc: 0.7967 - val_loss: 0.8624 - val_acc: 0.7730\n",
      "Epoch 954/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8296 - acc: 0.7955 - val_loss: 0.8666 - val_acc: 0.7700\n",
      "Epoch 955/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8304 - acc: 0.7936 - val_loss: 0.8820 - val_acc: 0.7740\n",
      "Epoch 956/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8294 - acc: 0.7951 - val_loss: 0.8776 - val_acc: 0.7760\n",
      "Epoch 957/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8311 - acc: 0.7921 - val_loss: 0.9091 - val_acc: 0.7580\n",
      "Epoch 958/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8301 - acc: 0.7952 - val_loss: 0.8609 - val_acc: 0.7770\n",
      "Epoch 959/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8293 - acc: 0.7951 - val_loss: 0.8673 - val_acc: 0.7700\n",
      "Epoch 960/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8315 - acc: 0.7940 - val_loss: 0.8695 - val_acc: 0.7740\n",
      "Epoch 961/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8308 - acc: 0.7948 - val_loss: 0.8662 - val_acc: 0.7700\n",
      "Epoch 962/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8289 - acc: 0.7959 - val_loss: 0.8629 - val_acc: 0.7800\n",
      "Epoch 963/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8269 - acc: 0.7956 - val_loss: 0.8621 - val_acc: 0.7790\n",
      "Epoch 964/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8309 - acc: 0.7948 - val_loss: 0.8687 - val_acc: 0.7730\n",
      "Epoch 965/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8297 - acc: 0.7956 - val_loss: 0.8656 - val_acc: 0.7730\n",
      "Epoch 966/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8275 - acc: 0.7956 - val_loss: 0.8643 - val_acc: 0.7700\n",
      "Epoch 967/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8281 - acc: 0.7937 - val_loss: 0.8645 - val_acc: 0.7780\n",
      "Epoch 968/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8269 - acc: 0.7956 - val_loss: 0.8661 - val_acc: 0.7810\n",
      "Epoch 969/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8286 - acc: 0.7937 - val_loss: 0.8648 - val_acc: 0.7770\n",
      "Epoch 970/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8291 - acc: 0.7952 - val_loss: 0.8656 - val_acc: 0.7810\n",
      "Epoch 971/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8297 - acc: 0.7952 - val_loss: 0.8651 - val_acc: 0.7770\n",
      "Epoch 972/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8281 - acc: 0.7944 - val_loss: 0.8798 - val_acc: 0.7690\n",
      "Epoch 973/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8282 - acc: 0.7987 - val_loss: 0.8627 - val_acc: 0.7750\n",
      "Epoch 974/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8275 - acc: 0.7961 - val_loss: 0.8654 - val_acc: 0.7710\n",
      "Epoch 975/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8281 - acc: 0.7956 - val_loss: 0.8720 - val_acc: 0.7670\n",
      "Epoch 976/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8328 - acc: 0.7943 - val_loss: 0.9056 - val_acc: 0.7610\n",
      "Epoch 977/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8312 - acc: 0.7955 - val_loss: 0.8746 - val_acc: 0.7730\n",
      "Epoch 978/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8295 - acc: 0.7947 - val_loss: 0.8683 - val_acc: 0.7670\n",
      "Epoch 979/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8262 - acc: 0.7971 - val_loss: 0.8672 - val_acc: 0.7750\n",
      "Epoch 980/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8269 - acc: 0.7943 - val_loss: 0.8624 - val_acc: 0.7740\n",
      "Epoch 981/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8274 - acc: 0.7972 - val_loss: 0.8839 - val_acc: 0.7690\n",
      "Epoch 982/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8274 - acc: 0.7949 - val_loss: 0.8687 - val_acc: 0.7670\n",
      "Epoch 983/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8260 - acc: 0.7971 - val_loss: 0.9091 - val_acc: 0.7640\n",
      "Epoch 984/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8273 - acc: 0.7960 - val_loss: 0.8981 - val_acc: 0.7720\n",
      "Epoch 985/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8323 - acc: 0.7923 - val_loss: 0.8765 - val_acc: 0.7760\n",
      "Epoch 986/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8276 - acc: 0.7931 - val_loss: 0.8668 - val_acc: 0.7680\n",
      "Epoch 987/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8286 - acc: 0.7952 - val_loss: 0.8701 - val_acc: 0.7690\n",
      "Epoch 988/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8278 - acc: 0.7963 - val_loss: 0.8693 - val_acc: 0.7620\n",
      "Epoch 989/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8265 - acc: 0.7953 - val_loss: 0.8649 - val_acc: 0.7690\n",
      "Epoch 990/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8269 - acc: 0.7981 - val_loss: 0.8839 - val_acc: 0.7730\n",
      "Epoch 991/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8277 - acc: 0.7940 - val_loss: 0.8716 - val_acc: 0.7770\n",
      "Epoch 992/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8268 - acc: 0.7971 - val_loss: 0.8685 - val_acc: 0.7830\n",
      "Epoch 993/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8255 - acc: 0.7952 - val_loss: 0.8832 - val_acc: 0.7540\n",
      "Epoch 994/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8265 - acc: 0.7960 - val_loss: 0.8651 - val_acc: 0.7650\n",
      "Epoch 995/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8261 - acc: 0.7964 - val_loss: 0.8932 - val_acc: 0.7690\n",
      "Epoch 996/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8298 - acc: 0.7944 - val_loss: 0.8595 - val_acc: 0.7830\n",
      "Epoch 997/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8243 - acc: 0.7968 - val_loss: 0.8693 - val_acc: 0.7730\n",
      "Epoch 998/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8247 - acc: 0.7968 - val_loss: 0.8596 - val_acc: 0.7790\n",
      "Epoch 999/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8254 - acc: 0.7988 - val_loss: 0.8680 - val_acc: 0.7660\n",
      "Epoch 1000/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8262 - acc: 0.7965 - val_loss: 0.8655 - val_acc: 0.7780\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 36us/step\n",
      "1500/1500 [==============================] - 0s 39us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.340783791732788, 0.7213333333333334]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.351107208251953, 0.7186666668256124]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 1s 116us/step - loss: 1.9913 - acc: 0.1251 - val_loss: 1.9466 - val_acc: 0.1370\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 1.9661 - acc: 0.1419 - val_loss: 1.9378 - val_acc: 0.1480\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.9493 - acc: 0.1573 - val_loss: 1.9318 - val_acc: 0.1840\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.9426 - acc: 0.1615 - val_loss: 1.9275 - val_acc: 0.1980\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.9361 - acc: 0.1708 - val_loss: 1.9232 - val_acc: 0.2020\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.9297 - acc: 0.1856 - val_loss: 1.9188 - val_acc: 0.2020\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.9242 - acc: 0.1875 - val_loss: 1.9139 - val_acc: 0.2060\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.9235 - acc: 0.1899 - val_loss: 1.9086 - val_acc: 0.2130\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.9143 - acc: 0.2069 - val_loss: 1.9032 - val_acc: 0.2240\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.9080 - acc: 0.2107 - val_loss: 1.8966 - val_acc: 0.2370\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.9038 - acc: 0.2163 - val_loss: 1.8891 - val_acc: 0.2420\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.8989 - acc: 0.2223 - val_loss: 1.8806 - val_acc: 0.2520\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.8886 - acc: 0.2268 - val_loss: 1.8698 - val_acc: 0.2610\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.8821 - acc: 0.2265 - val_loss: 1.8575 - val_acc: 0.2770\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.8692 - acc: 0.2455 - val_loss: 1.8437 - val_acc: 0.3040\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.8581 - acc: 0.2556 - val_loss: 1.8273 - val_acc: 0.3270\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.8541 - acc: 0.2563 - val_loss: 1.8121 - val_acc: 0.3410\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.8311 - acc: 0.2727 - val_loss: 1.7908 - val_acc: 0.3690\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.8244 - acc: 0.2717 - val_loss: 1.7711 - val_acc: 0.3890\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.8100 - acc: 0.2892 - val_loss: 1.7508 - val_acc: 0.4080\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.7834 - acc: 0.3031 - val_loss: 1.7254 - val_acc: 0.4260\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.7720 - acc: 0.3183 - val_loss: 1.7017 - val_acc: 0.4390\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.7546 - acc: 0.3193 - val_loss: 1.6761 - val_acc: 0.4610\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.7432 - acc: 0.3267 - val_loss: 1.6540 - val_acc: 0.4700\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.7143 - acc: 0.3480 - val_loss: 1.6259 - val_acc: 0.4860\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.6978 - acc: 0.3499 - val_loss: 1.5992 - val_acc: 0.5030\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.6803 - acc: 0.3545 - val_loss: 1.5731 - val_acc: 0.5100\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.6654 - acc: 0.3608 - val_loss: 1.5488 - val_acc: 0.5220\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.6428 - acc: 0.3753 - val_loss: 1.5207 - val_acc: 0.5340\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.6192 - acc: 0.3785 - val_loss: 1.4938 - val_acc: 0.5410\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.6015 - acc: 0.3917 - val_loss: 1.4690 - val_acc: 0.5490\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 1.5971 - acc: 0.3957 - val_loss: 1.4471 - val_acc: 0.5550\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.5780 - acc: 0.3924 - val_loss: 1.4235 - val_acc: 0.5690\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 1.5451 - acc: 0.4132 - val_loss: 1.3991 - val_acc: 0.5720\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 1.5346 - acc: 0.4143 - val_loss: 1.3773 - val_acc: 0.5850\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 1.5164 - acc: 0.4241 - val_loss: 1.3570 - val_acc: 0.5870\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 1.5011 - acc: 0.4359 - val_loss: 1.3350 - val_acc: 0.6040\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.4846 - acc: 0.4448 - val_loss: 1.3139 - val_acc: 0.6100\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.4779 - acc: 0.4339 - val_loss: 1.2961 - val_acc: 0.6200\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.4669 - acc: 0.4388 - val_loss: 1.2790 - val_acc: 0.6250\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 1.4431 - acc: 0.4524 - val_loss: 1.2601 - val_acc: 0.6340\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.4206 - acc: 0.4596 - val_loss: 1.2415 - val_acc: 0.6370\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 1.4202 - acc: 0.4560 - val_loss: 1.2253 - val_acc: 0.6480\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.3922 - acc: 0.4668 - val_loss: 1.2076 - val_acc: 0.6530\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.3880 - acc: 0.4687 - val_loss: 1.1934 - val_acc: 0.6570\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.3907 - acc: 0.4717 - val_loss: 1.1795 - val_acc: 0.6590\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 1.3614 - acc: 0.4839 - val_loss: 1.1651 - val_acc: 0.6590\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.3546 - acc: 0.4892 - val_loss: 1.1521 - val_acc: 0.6660\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.3447 - acc: 0.4924 - val_loss: 1.1396 - val_acc: 0.6690\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 1.3275 - acc: 0.4928 - val_loss: 1.1275 - val_acc: 0.6710\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 1.3217 - acc: 0.5024 - val_loss: 1.1133 - val_acc: 0.6750\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 1.3045 - acc: 0.5056 - val_loss: 1.0999 - val_acc: 0.6800\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.2925 - acc: 0.5057 - val_loss: 1.0847 - val_acc: 0.6760\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.2842 - acc: 0.5153 - val_loss: 1.0738 - val_acc: 0.6820\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 1s 96us/step - loss: 1.2739 - acc: 0.5147 - val_loss: 1.0596 - val_acc: 0.6880\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 1.2678 - acc: 0.5251 - val_loss: 1.0502 - val_acc: 0.6930\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.2619 - acc: 0.5188 - val_loss: 1.0415 - val_acc: 0.6990\n",
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.2469 - acc: 0.5315 - val_loss: 1.0308 - val_acc: 0.6970\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.2505 - acc: 0.5183 - val_loss: 1.0249 - val_acc: 0.7010\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.2331 - acc: 0.5388 - val_loss: 1.0160 - val_acc: 0.7000\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.2113 - acc: 0.5427 - val_loss: 1.0045 - val_acc: 0.7040\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.2198 - acc: 0.5441 - val_loss: 0.9978 - val_acc: 0.7050\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.2035 - acc: 0.5457 - val_loss: 0.9913 - val_acc: 0.7070\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.1899 - acc: 0.5516 - val_loss: 0.9806 - val_acc: 0.7160\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.1885 - acc: 0.5545 - val_loss: 0.9739 - val_acc: 0.7130\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.1712 - acc: 0.5583 - val_loss: 0.9636 - val_acc: 0.7140\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.1718 - acc: 0.5577 - val_loss: 0.9548 - val_acc: 0.7140\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.1648 - acc: 0.5615 - val_loss: 0.9453 - val_acc: 0.7180\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.1618 - acc: 0.5617 - val_loss: 0.9394 - val_acc: 0.7160\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.1529 - acc: 0.5653 - val_loss: 0.9340 - val_acc: 0.7200\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.1412 - acc: 0.5703 - val_loss: 0.9277 - val_acc: 0.7240\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.1409 - acc: 0.5680 - val_loss: 0.9181 - val_acc: 0.7270\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.1346 - acc: 0.5739 - val_loss: 0.9150 - val_acc: 0.7230\n",
      "Epoch 74/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.1325 - acc: 0.5665 - val_loss: 0.9073 - val_acc: 0.7240\n",
      "Epoch 75/200\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.1140 - acc: 0.5872 - val_loss: 0.9049 - val_acc: 0.7200\n",
      "Epoch 76/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.1092 - acc: 0.5812 - val_loss: 0.8932 - val_acc: 0.7280\n",
      "Epoch 77/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.0959 - acc: 0.5855 - val_loss: 0.8856 - val_acc: 0.7300\n",
      "Epoch 78/200\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.1095 - acc: 0.5856 - val_loss: 0.8843 - val_acc: 0.7310\n",
      "Epoch 79/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.1087 - acc: 0.5848 - val_loss: 0.8828 - val_acc: 0.7300\n",
      "Epoch 80/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.0940 - acc: 0.5932 - val_loss: 0.8754 - val_acc: 0.7310\n",
      "Epoch 81/200\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.0981 - acc: 0.5916 - val_loss: 0.8703 - val_acc: 0.7320\n",
      "Epoch 82/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.0695 - acc: 0.6031 - val_loss: 0.8641 - val_acc: 0.7340\n",
      "Epoch 83/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.0748 - acc: 0.5896 - val_loss: 0.8589 - val_acc: 0.7370\n",
      "Epoch 84/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.0591 - acc: 0.6024 - val_loss: 0.8541 - val_acc: 0.7330\n",
      "Epoch 85/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.0594 - acc: 0.6053 - val_loss: 0.8515 - val_acc: 0.7360\n",
      "Epoch 86/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.0498 - acc: 0.6047 - val_loss: 0.8448 - val_acc: 0.7370\n",
      "Epoch 87/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.0507 - acc: 0.6051 - val_loss: 0.8401 - val_acc: 0.7350\n",
      "Epoch 88/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.0530 - acc: 0.6053 - val_loss: 0.8362 - val_acc: 0.7360\n",
      "Epoch 89/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.0515 - acc: 0.6112 - val_loss: 0.8326 - val_acc: 0.7360\n",
      "Epoch 90/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.0413 - acc: 0.6075 - val_loss: 0.8279 - val_acc: 0.7370\n",
      "Epoch 91/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.0380 - acc: 0.6132 - val_loss: 0.8259 - val_acc: 0.7360\n",
      "Epoch 92/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.0234 - acc: 0.6195 - val_loss: 0.8210 - val_acc: 0.7360\n",
      "Epoch 93/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.0117 - acc: 0.6247 - val_loss: 0.8142 - val_acc: 0.7430\n",
      "Epoch 94/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.0335 - acc: 0.6171 - val_loss: 0.8126 - val_acc: 0.7440\n",
      "Epoch 95/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.0059 - acc: 0.6240 - val_loss: 0.8055 - val_acc: 0.7450\n",
      "Epoch 96/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.0135 - acc: 0.6195 - val_loss: 0.8043 - val_acc: 0.7440\n",
      "Epoch 97/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.9949 - acc: 0.6349 - val_loss: 0.7970 - val_acc: 0.7430\n",
      "Epoch 98/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9973 - acc: 0.6329 - val_loss: 0.7947 - val_acc: 0.7500\n",
      "Epoch 99/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.9995 - acc: 0.6272 - val_loss: 0.7906 - val_acc: 0.7490\n",
      "Epoch 100/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9876 - acc: 0.6268 - val_loss: 0.7871 - val_acc: 0.7450\n",
      "Epoch 101/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9818 - acc: 0.6363 - val_loss: 0.7846 - val_acc: 0.7500\n",
      "Epoch 102/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9852 - acc: 0.6331 - val_loss: 0.7811 - val_acc: 0.7460\n",
      "Epoch 103/200\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.9726 - acc: 0.6425 - val_loss: 0.7794 - val_acc: 0.7520\n",
      "Epoch 104/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9796 - acc: 0.6361 - val_loss: 0.7768 - val_acc: 0.7510\n",
      "Epoch 105/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9854 - acc: 0.6407 - val_loss: 0.7774 - val_acc: 0.7490\n",
      "Epoch 106/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9722 - acc: 0.6439 - val_loss: 0.7729 - val_acc: 0.7490\n",
      "Epoch 107/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9673 - acc: 0.6396 - val_loss: 0.7687 - val_acc: 0.7540\n",
      "Epoch 108/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.9616 - acc: 0.6425 - val_loss: 0.7655 - val_acc: 0.7530\n",
      "Epoch 109/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9727 - acc: 0.6320 - val_loss: 0.7637 - val_acc: 0.7510\n",
      "Epoch 110/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.9633 - acc: 0.6460 - val_loss: 0.7589 - val_acc: 0.7490\n",
      "Epoch 111/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.9452 - acc: 0.6491 - val_loss: 0.7543 - val_acc: 0.7520\n",
      "Epoch 112/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.9439 - acc: 0.6451 - val_loss: 0.7533 - val_acc: 0.7500\n",
      "Epoch 113/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.9430 - acc: 0.6475 - val_loss: 0.7503 - val_acc: 0.7520\n",
      "Epoch 114/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.9286 - acc: 0.6508 - val_loss: 0.7460 - val_acc: 0.7560\n",
      "Epoch 115/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.9416 - acc: 0.6524 - val_loss: 0.7447 - val_acc: 0.7570\n",
      "Epoch 116/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.9322 - acc: 0.6576 - val_loss: 0.7432 - val_acc: 0.7540\n",
      "Epoch 117/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.9268 - acc: 0.6539 - val_loss: 0.7393 - val_acc: 0.7540\n",
      "Epoch 118/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.9231 - acc: 0.6619 - val_loss: 0.7375 - val_acc: 0.7540\n",
      "Epoch 119/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.9246 - acc: 0.6571 - val_loss: 0.7369 - val_acc: 0.7550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.9281 - acc: 0.6580 - val_loss: 0.7365 - val_acc: 0.7520\n",
      "Epoch 121/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9185 - acc: 0.6563 - val_loss: 0.7306 - val_acc: 0.7580\n",
      "Epoch 122/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9158 - acc: 0.6575 - val_loss: 0.7300 - val_acc: 0.7570\n",
      "Epoch 123/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.9091 - acc: 0.6647 - val_loss: 0.7270 - val_acc: 0.7540\n",
      "Epoch 124/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9074 - acc: 0.6628 - val_loss: 0.7254 - val_acc: 0.7550\n",
      "Epoch 125/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.9061 - acc: 0.6603 - val_loss: 0.7235 - val_acc: 0.7570\n",
      "Epoch 126/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8961 - acc: 0.6703 - val_loss: 0.7208 - val_acc: 0.7580\n",
      "Epoch 127/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.9025 - acc: 0.6687 - val_loss: 0.7197 - val_acc: 0.7560\n",
      "Epoch 128/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8909 - acc: 0.6656 - val_loss: 0.7165 - val_acc: 0.7550\n",
      "Epoch 129/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8857 - acc: 0.6673 - val_loss: 0.7150 - val_acc: 0.7610\n",
      "Epoch 130/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8884 - acc: 0.6771 - val_loss: 0.7133 - val_acc: 0.7600\n",
      "Epoch 131/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.8923 - acc: 0.6717 - val_loss: 0.7122 - val_acc: 0.7570\n",
      "Epoch 132/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8721 - acc: 0.6772 - val_loss: 0.7082 - val_acc: 0.7560\n",
      "Epoch 133/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.8753 - acc: 0.6673 - val_loss: 0.7068 - val_acc: 0.7580\n",
      "Epoch 134/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.8804 - acc: 0.6693 - val_loss: 0.7057 - val_acc: 0.7570\n",
      "Epoch 135/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8708 - acc: 0.6761 - val_loss: 0.7027 - val_acc: 0.7610\n",
      "Epoch 136/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.8694 - acc: 0.6860 - val_loss: 0.7019 - val_acc: 0.7590\n",
      "Epoch 137/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.8744 - acc: 0.6760 - val_loss: 0.7015 - val_acc: 0.7590\n",
      "Epoch 138/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.8612 - acc: 0.6844 - val_loss: 0.7006 - val_acc: 0.7610\n",
      "Epoch 139/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8651 - acc: 0.6757 - val_loss: 0.7001 - val_acc: 0.7590\n",
      "Epoch 140/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8518 - acc: 0.6799 - val_loss: 0.6977 - val_acc: 0.7600\n",
      "Epoch 141/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8612 - acc: 0.6799 - val_loss: 0.6943 - val_acc: 0.7610\n",
      "Epoch 142/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.8539 - acc: 0.6769 - val_loss: 0.6920 - val_acc: 0.7610\n",
      "Epoch 143/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.8545 - acc: 0.6803 - val_loss: 0.6922 - val_acc: 0.7580\n",
      "Epoch 144/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8554 - acc: 0.6827 - val_loss: 0.6920 - val_acc: 0.7610\n",
      "Epoch 145/200\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.8374 - acc: 0.6937 - val_loss: 0.6884 - val_acc: 0.7660\n",
      "Epoch 146/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8509 - acc: 0.6819 - val_loss: 0.6870 - val_acc: 0.7650\n",
      "Epoch 147/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.8461 - acc: 0.6839 - val_loss: 0.6876 - val_acc: 0.7660\n",
      "Epoch 148/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8420 - acc: 0.6899 - val_loss: 0.6873 - val_acc: 0.7640\n",
      "Epoch 149/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8403 - acc: 0.6840 - val_loss: 0.6846 - val_acc: 0.7640\n",
      "Epoch 150/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.8402 - acc: 0.6895 - val_loss: 0.6833 - val_acc: 0.7640\n",
      "Epoch 151/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.8359 - acc: 0.6879 - val_loss: 0.6842 - val_acc: 0.7640\n",
      "Epoch 152/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8431 - acc: 0.6825 - val_loss: 0.6828 - val_acc: 0.7640\n",
      "Epoch 153/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8200 - acc: 0.6927 - val_loss: 0.6807 - val_acc: 0.7660\n",
      "Epoch 154/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.8334 - acc: 0.6896 - val_loss: 0.6783 - val_acc: 0.7680\n",
      "Epoch 155/200\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.8329 - acc: 0.6879 - val_loss: 0.6777 - val_acc: 0.7660\n",
      "Epoch 156/200\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 0.8213 - acc: 0.6956 - val_loss: 0.6748 - val_acc: 0.7730\n",
      "Epoch 157/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8363 - acc: 0.6897 - val_loss: 0.6744 - val_acc: 0.7700\n",
      "Epoch 158/200\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.8213 - acc: 0.6940 - val_loss: 0.6737 - val_acc: 0.7690\n",
      "Epoch 159/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8252 - acc: 0.6900 - val_loss: 0.6726 - val_acc: 0.7660\n",
      "Epoch 160/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.8120 - acc: 0.6993 - val_loss: 0.6721 - val_acc: 0.7660\n",
      "Epoch 161/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8188 - acc: 0.6909 - val_loss: 0.6714 - val_acc: 0.7700\n",
      "Epoch 162/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.8151 - acc: 0.6915 - val_loss: 0.6701 - val_acc: 0.7700\n",
      "Epoch 163/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.7917 - acc: 0.7079 - val_loss: 0.6660 - val_acc: 0.7690\n",
      "Epoch 164/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.8046 - acc: 0.6992 - val_loss: 0.6664 - val_acc: 0.7710\n",
      "Epoch 165/200\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.8049 - acc: 0.6971 - val_loss: 0.6672 - val_acc: 0.7770\n",
      "Epoch 166/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.7991 - acc: 0.7035 - val_loss: 0.6667 - val_acc: 0.7760\n",
      "Epoch 167/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.8096 - acc: 0.6995 - val_loss: 0.6638 - val_acc: 0.7700\n",
      "Epoch 168/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.8085 - acc: 0.6927 - val_loss: 0.6636 - val_acc: 0.7710\n",
      "Epoch 169/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.7985 - acc: 0.6989 - val_loss: 0.6626 - val_acc: 0.7700\n",
      "Epoch 170/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.8084 - acc: 0.7015 - val_loss: 0.6628 - val_acc: 0.7740\n",
      "Epoch 171/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.7930 - acc: 0.7051 - val_loss: 0.6605 - val_acc: 0.7750\n",
      "Epoch 172/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.7945 - acc: 0.7036 - val_loss: 0.6600 - val_acc: 0.7700\n",
      "Epoch 173/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.7961 - acc: 0.7057 - val_loss: 0.6598 - val_acc: 0.7730\n",
      "Epoch 174/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.7907 - acc: 0.7080 - val_loss: 0.6575 - val_acc: 0.7770\n",
      "Epoch 175/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.7733 - acc: 0.7099 - val_loss: 0.6564 - val_acc: 0.7750\n",
      "Epoch 176/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.7769 - acc: 0.7079 - val_loss: 0.6570 - val_acc: 0.7760\n",
      "Epoch 177/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.7883 - acc: 0.7064 - val_loss: 0.6563 - val_acc: 0.7760\n",
      "Epoch 178/200\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.7794 - acc: 0.7103 - val_loss: 0.6517 - val_acc: 0.7760\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.7769 - acc: 0.7060 - val_loss: 0.6531 - val_acc: 0.7770\n",
      "Epoch 180/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.7690 - acc: 0.7088 - val_loss: 0.6524 - val_acc: 0.7770\n",
      "Epoch 181/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.7756 - acc: 0.7069 - val_loss: 0.6498 - val_acc: 0.7800\n",
      "Epoch 182/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.7761 - acc: 0.7119 - val_loss: 0.6509 - val_acc: 0.7800\n",
      "Epoch 183/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.7825 - acc: 0.7041 - val_loss: 0.6497 - val_acc: 0.7790\n",
      "Epoch 184/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.7652 - acc: 0.7129 - val_loss: 0.6478 - val_acc: 0.7800\n",
      "Epoch 185/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.7801 - acc: 0.7184 - val_loss: 0.6483 - val_acc: 0.7800\n",
      "Epoch 186/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.7695 - acc: 0.7137 - val_loss: 0.6469 - val_acc: 0.7760\n",
      "Epoch 187/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.7572 - acc: 0.7173 - val_loss: 0.6459 - val_acc: 0.7750\n",
      "Epoch 188/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.7743 - acc: 0.7087 - val_loss: 0.6456 - val_acc: 0.7810\n",
      "Epoch 189/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.7585 - acc: 0.7171 - val_loss: 0.6434 - val_acc: 0.7790\n",
      "Epoch 190/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.7529 - acc: 0.7156 - val_loss: 0.6421 - val_acc: 0.7820\n",
      "Epoch 191/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.7612 - acc: 0.7141 - val_loss: 0.6437 - val_acc: 0.7810\n",
      "Epoch 192/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.7476 - acc: 0.7159 - val_loss: 0.6430 - val_acc: 0.7800\n",
      "Epoch 193/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.7434 - acc: 0.7223 - val_loss: 0.6411 - val_acc: 0.7770\n",
      "Epoch 194/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.7671 - acc: 0.7121 - val_loss: 0.6433 - val_acc: 0.7800\n",
      "Epoch 195/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.7635 - acc: 0.7163 - val_loss: 0.6414 - val_acc: 0.7800\n",
      "Epoch 196/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.7517 - acc: 0.7165 - val_loss: 0.6415 - val_acc: 0.7840\n",
      "Epoch 197/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.7546 - acc: 0.7153 - val_loss: 0.6402 - val_acc: 0.7780\n",
      "Epoch 198/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.7627 - acc: 0.7200 - val_loss: 0.6421 - val_acc: 0.7810\n",
      "Epoch 199/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.7447 - acc: 0.7181 - val_loss: 0.6408 - val_acc: 0.7820\n",
      "Epoch 200/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.7516 - acc: 0.7187 - val_loss: 0.6417 - val_acc: 0.7800\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 24us/step\n",
      "1500/1500 [==============================] - 0s 26us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.44953240927060445, 0.8355999999682109]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6567809325853984, 0.745333333492279]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 1.9131 - acc: 0.1977 - val_loss: 1.8734 - val_acc: 0.2517\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 1.8204 - acc: 0.3034 - val_loss: 1.7551 - val_acc: 0.3397\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 1.6686 - acc: 0.4072 - val_loss: 1.5741 - val_acc: 0.4647\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 1.4662 - acc: 0.5248 - val_loss: 1.3619 - val_acc: 0.5560\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 1.2557 - acc: 0.6060 - val_loss: 1.1666 - val_acc: 0.6303\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 1.0768 - acc: 0.6660 - val_loss: 1.0120 - val_acc: 0.6777\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.9451 - acc: 0.7012 - val_loss: 0.9037 - val_acc: 0.7047\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.8536 - acc: 0.7191 - val_loss: 0.8281 - val_acc: 0.7210\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.7894 - acc: 0.7321 - val_loss: 0.7750 - val_acc: 0.7300\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.7434 - acc: 0.7419 - val_loss: 0.7363 - val_acc: 0.7397\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.7086 - acc: 0.7492 - val_loss: 0.7075 - val_acc: 0.7450\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.6815 - acc: 0.7555 - val_loss: 0.6846 - val_acc: 0.7507\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.6596 - acc: 0.7612 - val_loss: 0.6661 - val_acc: 0.7587\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.6413 - acc: 0.7678 - val_loss: 0.6501 - val_acc: 0.7623\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.6259 - acc: 0.7724 - val_loss: 0.6383 - val_acc: 0.7653\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.6119 - acc: 0.7780 - val_loss: 0.6275 - val_acc: 0.7700\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5999 - acc: 0.7809 - val_loss: 0.6168 - val_acc: 0.7720\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5891 - acc: 0.7845 - val_loss: 0.6079 - val_acc: 0.7757\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5792 - acc: 0.7892 - val_loss: 0.6013 - val_acc: 0.7753\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5700 - acc: 0.7920 - val_loss: 0.5924 - val_acc: 0.7813\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5614 - acc: 0.7954 - val_loss: 0.5879 - val_acc: 0.7807\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5535 - acc: 0.7989 - val_loss: 0.5832 - val_acc: 0.7817\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5459 - acc: 0.8010 - val_loss: 0.5766 - val_acc: 0.7847\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5391 - acc: 0.8044 - val_loss: 0.5735 - val_acc: 0.7850\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5326 - acc: 0.8076 - val_loss: 0.5674 - val_acc: 0.7937\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5264 - acc: 0.8092 - val_loss: 0.5622 - val_acc: 0.7920\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5198 - acc: 0.8114 - val_loss: 0.5599 - val_acc: 0.7977\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5144 - acc: 0.8140 - val_loss: 0.5571 - val_acc: 0.8000\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5087 - acc: 0.8162 - val_loss: 0.5509 - val_acc: 0.8000\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5033 - acc: 0.8180 - val_loss: 0.5483 - val_acc: 0.8020\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4982 - acc: 0.8205 - val_loss: 0.5443 - val_acc: 0.8023\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4934 - acc: 0.8222 - val_loss: 0.5435 - val_acc: 0.8027\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4885 - acc: 0.8253 - val_loss: 0.5426 - val_acc: 0.8033\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4840 - acc: 0.8256 - val_loss: 0.5386 - val_acc: 0.8080\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4799 - acc: 0.8278 - val_loss: 0.5341 - val_acc: 0.8093\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4755 - acc: 0.8305 - val_loss: 0.5322 - val_acc: 0.8100\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4713 - acc: 0.8308 - val_loss: 0.5297 - val_acc: 0.8117\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4674 - acc: 0.8319 - val_loss: 0.5273 - val_acc: 0.8123\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4632 - acc: 0.8339 - val_loss: 0.5265 - val_acc: 0.8103\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4599 - acc: 0.8355 - val_loss: 0.5236 - val_acc: 0.8103\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4558 - acc: 0.8370 - val_loss: 0.5241 - val_acc: 0.8103\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4522 - acc: 0.8383 - val_loss: 0.5210 - val_acc: 0.8120\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4487 - acc: 0.8403 - val_loss: 0.5223 - val_acc: 0.8143\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4453 - acc: 0.8405 - val_loss: 0.5187 - val_acc: 0.8180\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4420 - acc: 0.8427 - val_loss: 0.5206 - val_acc: 0.8153\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4388 - acc: 0.8437 - val_loss: 0.5186 - val_acc: 0.8120\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4358 - acc: 0.8442 - val_loss: 0.5154 - val_acc: 0.8133\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4324 - acc: 0.8458 - val_loss: 0.5156 - val_acc: 0.8130\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4296 - acc: 0.8474 - val_loss: 0.5147 - val_acc: 0.8150\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4266 - acc: 0.8484 - val_loss: 0.5136 - val_acc: 0.8117\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4235 - acc: 0.8492 - val_loss: 0.5142 - val_acc: 0.8167\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4206 - acc: 0.8502 - val_loss: 0.5135 - val_acc: 0.8133\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4181 - acc: 0.8508 - val_loss: 0.5154 - val_acc: 0.8163\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4152 - acc: 0.8521 - val_loss: 0.5109 - val_acc: 0.8140\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4125 - acc: 0.8531 - val_loss: 0.5124 - val_acc: 0.8160\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4100 - acc: 0.8537 - val_loss: 0.5126 - val_acc: 0.8163\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4076 - acc: 0.8543 - val_loss: 0.5112 - val_acc: 0.8180\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4052 - acc: 0.8555 - val_loss: 0.5120 - val_acc: 0.8113\n",
      "Epoch 59/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4028 - acc: 0.8556 - val_loss: 0.5111 - val_acc: 0.8130\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4000 - acc: 0.8580 - val_loss: 0.5105 - val_acc: 0.8183\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3977 - acc: 0.8590 - val_loss: 0.5116 - val_acc: 0.8163\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3953 - acc: 0.8602 - val_loss: 0.5123 - val_acc: 0.8180\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3931 - acc: 0.8606 - val_loss: 0.5089 - val_acc: 0.8157\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3908 - acc: 0.8617 - val_loss: 0.5120 - val_acc: 0.8113\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3888 - acc: 0.8624 - val_loss: 0.5128 - val_acc: 0.8157\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3863 - acc: 0.8633 - val_loss: 0.5122 - val_acc: 0.8167\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3844 - acc: 0.8638 - val_loss: 0.5100 - val_acc: 0.8157\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3821 - acc: 0.8646 - val_loss: 0.5113 - val_acc: 0.8160\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3801 - acc: 0.8665 - val_loss: 0.5136 - val_acc: 0.8120\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3779 - acc: 0.8661 - val_loss: 0.5121 - val_acc: 0.8163\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3760 - acc: 0.8688 - val_loss: 0.5113 - val_acc: 0.8117\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3741 - acc: 0.8685 - val_loss: 0.5115 - val_acc: 0.8163\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3720 - acc: 0.8689 - val_loss: 0.5121 - val_acc: 0.8167\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3702 - acc: 0.8699 - val_loss: 0.5157 - val_acc: 0.8160\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3681 - acc: 0.8702 - val_loss: 0.5137 - val_acc: 0.8160\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3661 - acc: 0.8725 - val_loss: 0.5126 - val_acc: 0.8143\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3641 - acc: 0.8718 - val_loss: 0.5138 - val_acc: 0.8147\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3627 - acc: 0.8725 - val_loss: 0.5194 - val_acc: 0.8160\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3610 - acc: 0.8739 - val_loss: 0.5152 - val_acc: 0.8117\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3589 - acc: 0.8742 - val_loss: 0.5166 - val_acc: 0.8170\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3571 - acc: 0.8754 - val_loss: 0.5157 - val_acc: 0.8147\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3557 - acc: 0.8765 - val_loss: 0.5159 - val_acc: 0.8150\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3536 - acc: 0.8771 - val_loss: 0.5180 - val_acc: 0.8157\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3520 - acc: 0.8768 - val_loss: 0.5189 - val_acc: 0.8140\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3501 - acc: 0.8779 - val_loss: 0.5177 - val_acc: 0.8160\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3487 - acc: 0.8785 - val_loss: 0.5218 - val_acc: 0.8167\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3468 - acc: 0.8794 - val_loss: 0.5212 - val_acc: 0.8137\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3456 - acc: 0.8793 - val_loss: 0.5198 - val_acc: 0.8153\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3438 - acc: 0.8801 - val_loss: 0.5210 - val_acc: 0.8143\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3421 - acc: 0.8802 - val_loss: 0.5235 - val_acc: 0.8127\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3406 - acc: 0.8813 - val_loss: 0.5213 - val_acc: 0.8143\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3386 - acc: 0.8818 - val_loss: 0.5223 - val_acc: 0.8153\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3374 - acc: 0.8827 - val_loss: 0.5232 - val_acc: 0.8137\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3358 - acc: 0.8840 - val_loss: 0.5240 - val_acc: 0.8150\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3342 - acc: 0.8832 - val_loss: 0.5284 - val_acc: 0.8160\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3328 - acc: 0.8852 - val_loss: 0.5263 - val_acc: 0.8160\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3312 - acc: 0.8856 - val_loss: 0.5260 - val_acc: 0.8137\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3297 - acc: 0.8869 - val_loss: 0.5322 - val_acc: 0.8117\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3280 - acc: 0.8863 - val_loss: 0.5297 - val_acc: 0.8140\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3267 - acc: 0.8873 - val_loss: 0.5302 - val_acc: 0.8127\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3257 - acc: 0.8878 - val_loss: 0.5295 - val_acc: 0.8133\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3239 - acc: 0.8889 - val_loss: 0.5335 - val_acc: 0.8143\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3223 - acc: 0.8888 - val_loss: 0.5320 - val_acc: 0.8153\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3212 - acc: 0.8890 - val_loss: 0.5335 - val_acc: 0.8130\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3196 - acc: 0.8891 - val_loss: 0.5339 - val_acc: 0.8150\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3184 - acc: 0.8903 - val_loss: 0.5370 - val_acc: 0.8143\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3171 - acc: 0.8912 - val_loss: 0.5352 - val_acc: 0.8147\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3152 - acc: 0.8909 - val_loss: 0.5379 - val_acc: 0.8127\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3144 - acc: 0.8923 - val_loss: 0.5363 - val_acc: 0.8137\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3130 - acc: 0.8931 - val_loss: 0.5379 - val_acc: 0.8133\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3114 - acc: 0.8927 - val_loss: 0.5388 - val_acc: 0.8153\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3100 - acc: 0.8924 - val_loss: 0.5392 - val_acc: 0.8147\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3091 - acc: 0.8947 - val_loss: 0.5406 - val_acc: 0.8137\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3078 - acc: 0.8943 - val_loss: 0.5422 - val_acc: 0.8157\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3061 - acc: 0.8951 - val_loss: 0.5433 - val_acc: 0.8123\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3052 - acc: 0.8956 - val_loss: 0.5432 - val_acc: 0.8130\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3035 - acc: 0.8950 - val_loss: 0.5483 - val_acc: 0.8090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3024 - acc: 0.8965 - val_loss: 0.5461 - val_acc: 0.8117\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3013 - acc: 0.8966 - val_loss: 0.5459 - val_acc: 0.8127\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.2998 - acc: 0.8972 - val_loss: 0.5460 - val_acc: 0.8150\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 35us/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-77d72b37b950>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mresults_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_final\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_train_final\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mresults_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
